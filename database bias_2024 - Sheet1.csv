Speaker,Time,Date,Duration,Statement,Trimmed Statement,rac,gender,discriminat,bias,automat,cognitive,user,interact,deploy,data,algorithm,program,develop,process,societ,existing,histor,selection,measures
AUSTRIA,15:59:29,2024-03-06,0:04:12,"We would just have a short remark on the overall risk. I think it is a very good question to pose to identify the risks that are there because in the past we have talked a lot about risk mitigation measures without clearly identifying the risks. We very much welcome also a debate about this topic. At the same time when we look at the list, we see that some of them are classic risks but some of them as they are listed here also are more something that comes from the lack or not implementing one of the measures that are listed in other parts of this document of our discussion. So not doing what we see as the proper way forward in addressing the risk surrounding autonomous weapon systems is not a risk per se but this is just something of a side note. What we see I think what might be useful if we have this debate is to do a little bit of a grouping. What we see are like Mexico before us, a set of broader security risks that are also more broadly linked to our work here. There is an overall humanitarian concern as the risk of humans losing control of armed conflict while it is humans who will continue to bear the consequences of armed conflict. This has been mentioned in the past already through a number of Delegations also that there is a risk towards peace and security and a possible arms race. There is the risk of proliferation as to use of autonomous weapon systems by non-state actors seems a very high risk once the Pandora's box is open. There is also as Mexico has mentioned a risk from integrating AI in broader weapon systems but I think we have addressed this in our discussion when we have discussed AI as functions of other weapon systems. Closest to our work are probably those risks that relate to IHL implementation and those are, of course, unintended engagement, unpredictability, incidental loss of life, injuries to civilians and damage to civilian objects resulting from the use of weapon systems as such, loss of control of the system. And in this regard we also want to stress that the risks of unpredictability of a weapon systems caused by self learning and the black box problem would not only violate IHL but could impact the military utilities since no military has use for weapon systems it cannot control. There are specific security risks related to the weapon systems, cyber threats, spoofing, using other AI specific vulnerabilities and then there is this more broader AI risk list that has also already been mentioned that relates to, again, the black box problem, automation bias, discrimination for building bias and risks related to datasets, risks regarding the integrity, quality and veracity of data that is used. This also relates a little bit to the interaction between the military or the state side and the private sector. And then there is also a risk that stems from a lack of adequate training of personnel on all relevant levels and on a more broader scale also the skills degradation. These are just some general risks that we wanted to list under this topic. Many of those risks can be mitigated or even eliminated by human control and others, this is also clear, would mostly be mitigated through legal reviews, through measures on the national level as well. There is many risks but also many different ways on addressing them. In our view, the human control is already a very big aspect of this and does a lot of good work. Thank you.","[...] There are specific security risks related to the weapon systems, cyber threats, spoofing, using other AI specific vulnerabilities and then there is this more broader AI risk list that has also already been mentioned that relates to, again, the black box problem, automation bias, discrimination for building bias and risks related to datasets, risks regarding the integrity, quality and veracity of data that is used. This also relates a little bit to the interaction between the military or the state side and the private sector. [...] ",x,,x,x,x,,,x,,x,,,,,,,,,
SWITZERLAND,11:20:46,2024-08-30,0:03:46,"Thank you, Mr. Chair, and thank you for this good section on other measures to ensure compliance with IHL. With regard to the first paragraph, we strongly support inclusion of testing and evaluation. This also has a connection to legal reviews, and we fully support what our Brazilian colleague just said about testing during actual operation. That is certainly something we would not like to see, but rather in realistic scenarios. Depending on the future instrument, the term should would need in our view to be adapted to must, should not just be a recommendation. With regard to paragraph 2, we think this is an important aspect and crucial for ensuring that means and methods of warfare can be used legally. Legal reviews are an obligation for state parties to additional protocol 1 and more broadly of the principle of pactas and servanda and obligation to respect with IHL. States are required to access if and under what circumstances means and methods of warfare can legally be used. The term should seems to us to weak in our view if we want to state that it is in fact a requirement. Moreover, with regard to possible legal use, the conduct of legal reviews cannot only aim to understand the capabilities and limitations of an autonomous weapon system. A legal review of a weapon must assess whether a potential use of the specific weapon system is compatible with international law. Where this is only the case under certain conditions or limitations, the legal review must define specific conditions or limitations that must be taken into account during use. With regard to paragraph 3, this is really good and we support it because unwanted data bias results in discriminatory outcomes. We also support suggestions to add that unwanted bias must be eliminated or eradicated as much as possible. Data can reflect different social discriminations and can then be reproduced in such weapon systems. This should be avoided. We also support the paragraph, the next paragraph but experience shows that there is often automation bias and we would need to be more specific how and which measures can be implemented. Finally, an additional useful measure that we would propose is in our view to require that states and parties to arm conflict establish clear policies, doctrine, procedures and instructions on the use of these autonomous weapon systems. Thank you, Mr. Chair.","[...] With regard to paragraph 3, this is really good and we support it because unwanted data bias results in discriminatory outcomes. We also support suggestions to add that unwanted bias must be eliminated or eradicated as much as possible. Data can reflect different social discriminations and can then be reproduced in such weapon systems. This should be avoided. We also support the paragraph, the next paragraph but experience shows that there is often automation bias and we would need to be more specific how and which measures can be implemented. [...]",,,x,x,x,,,,,x,,,,,,,,,
UNITED KINGDOM,12:00:36,2024-08-30,0:03:32,"Thank you, Mr. Chair. The UK welcomes this section and the approach that we are seeing in this room to make the important measures which ensure that throughout development acquisition and adoption of capabilities and follow on from the measures in the previous section that speak of compliance and use. The UK agrees with the structural points made by the ICRC in terms of the overall obligations in respect to the performance of legal reviews and its technical provision including the need for rigorous testing, evaluation, validation and verification. Going through this section as it is currently structured, the UK would suggest that we do the following. The UK would suggest that the term human operator might be unhelpful. If we are addressing the need for the commander to understand the weapons performance, we are referring to a human user. Nevertheless, the need to understand the weapons performance is a requirement that feeds into the manner in which the weapon is fielded and deployed. We would therefore propose the statement would be stronger if it read as the following, and I quote, we can send this to you later. States should ensure that rigorous testing and evaluation is conducted to ensure users and operators have a reliable expectation of how the weapon system will perform in the anticipated circumstances of its use. In regards to the conduct of legal reviews, we believe that we should ensure that the system in its specific design and expectation of performance is reviewed ahead of deployment. Therefore, we would propose to reflect in the text as thus, ahead of any deployment of laws, states should conduct legal reviews to understand the system or any modification to the system's capabilities and limitations, expected circumstances of use and its anticipated effects in different circumstances. We would also be amenable to the suggestions to more closely reflect the nature of Article 36 in this section. Moving to paragraphs 3 and 4, we are not against the exclusion of the phrase unwanted from paragraph 3 as suggested by the high contracting parties. We are aware bias is going to be present in all systems and the risk presented must be suitably mitigated as appropriate to the context of use. However, we do know if the word unwanted is removed, we must then also remove the word possible. All datasets would contain elements of bias so the term possible bias would be a mischaracterization and potentially confusing. We would also potentially favor a more active description of addressing bias which we know will be present rather than simply reviewing its presence. We will review some wording and submit it to you. On the section on automation bias, the UK Delegation is unclear on the necessity to link the concept of automation bias to the term unwanted as automation bias is inherently a negative trait that should be considered unwanted. Automation bias is defined as the propensity for humans to favor suggestions from automated decision making systems and to ignore contradictory information made without automation even if it is correct. Context appropriate human control should be applied at all times and automation bias presents a risk to the maintenance of this. Thank you.","[...] Moving to paragraphs 3 and 4, we are not against the exclusion of the phrase unwanted from paragraph 3 as suggested by the high contracting parties. We are aware bias is going to be present in all systems and the risk presented must be suitably mitigated as appropriate to the context of use. However, we do know if the word unwanted is removed, we must then also remove the word possible. All datasets would contain elements of bias so the term possible bias would be a mischaracterization and potentially confusing. We would also potentially favor a more active description of addressing bias which we know will be present rather than simply reviewing its presence. We will review some wording and submit it to you. On the section on automation bias, the UK Delegation is unclear on the necessity to link the concept of automation bias to the term unwanted as automation bias is inherently a negative trait that should be considered unwanted. Automation bias is defined as the propensity for humans to favor suggestions from automated decision making systems and to ignore contradictory information made without automation even if it is correct. Context appropriate human control should be applied at all times and automation bias presents a risk to the maintenance of this. [...]",x,,,x,x,,,,,x,,,,,,,,,
ENCODE JUSTICE,11:03:30,2024-08-30,0:03:06,"We firstly want to applaud the Delegations of Austria, Belgium, Canada, Costa Rica, Germany, Ireland, Luxembourg, Mexico, Panama and Uruguay for addressing in their working document the serious risks that the result of bias in laws could pose. And the Delegations of Canada, Germany and Ireland for their initiative to discuss this further in the site event organized last Wednesday. We want to dive into bias a little further. I will immediately clarify that I will take the technological definition of bias rather than the diplomatic definition, meaning that in the technological understanding there is no such thing as intended bias. The reference in the working document to bias in gender and ethnicity would be what data scientists call pre-existing bias, referring to bias that already exists in society and is thus replicated and often enlarged in an algorithm. We want to refer to our point made before that the self-learning capabilities of AI are solely based on historical data. Hence, without, again, periodic reassessment of evaluation measures, it is impossible to mitigate the risks of pre-existing bias. Again, it is a point we have made before. A new one we want to bring forward is drawing the attention to other types of bias that we know Mr. Chairman, could you kindly request the speaker to slow down? Thank you. Apologies. That will immediately clarify some aspects brought forward by the Russian Federation, that paragraph 3 and 4 do not completely overlap. The first type of bias we know is technical bias, including decontextualized algorithms and the second one is emergent bias caused by changes in societal knowledge, population or cultural values. Once again, a regular reassessment of evaluation measures is the absolute minimum that should be included in any instrument mitigating bias. We want to stress again that there is no such thing as an unbiased system. However, there are ways to mitigate these biases that will at least minimize the system error as much as possible. In the case of automated weapon systems, this does mean we are speaking of saving innocent human lives. However, we can only save them if the evaluation measures that will be required from a system are not set only throughout the life cycle of an AWS but also throughout the design cycle of the system itself. Hoping that third time is a charm, we want to repeat our call for periodic reassessment of evaluation measures and include not only the life cycle but also the design cycle of a system in mitigating biases. Lastly, it is not just about bias in datasets or automation bias as currently expressed in paragraph 3 and 4 respectively, but also about technical bias and emerging bias. Thank you.","[...] We want to dive into bias a little further. I will immediately clarify that I will take the technological definition of bias rather than the diplomatic definition, meaning that in the technological understanding there is no such thing as intended bias. The reference in the working document to bias in gender and ethnicity would be what data scientists call pre-existing bias, referring to bias that already exists in society and is thus replicated and often enlarged in an algorithm. We want to refer to our point made before that the self - learning capabilities of AI are solely based on historical data. Hence, without, again, periodic reassessment of evaluation measures, it is impossible to mitigate the risks of pre-existing bias. Again, it is a point we have made before. A new one we want to bring forward is drawing the attention to other types of bias that we know Mr. Chairman, could you kindly request the speaker to slow down? Thank you. Apologies. That will immediately clarify some aspects brought forward by the Russian Federation, that paragraph 3 and 4 do not completely overlap. The first type of bias we know is technical bias, including decontextualized algorithms and the second one is emergent bias caused by changes in societal knowledge, population or cultural values. Once again, a regular reassessment of evaluation measures is the absolute minimum that should be included in any instrument mitigating bias. We want to stress again that there is no such thing as an unbiased system. However, there are ways to mitigate these biases that will at least minimize the system error as much as possible. In the case of automated weapon systems, this does mean we are speaking of saving innocent human lives. However, we can only save them if the evaluation measures that will be required from a system are not set only throughout the life cycle of an AWS but also throughout the design cycle of the system itself. Hoping that third time is a charm, we want to repeat our call for periodic reassessment of evaluation measures and include not only the life cycle but also the design cycle of a system in mitigating biases. Lastly, it is not just about bias in datasets or automation bias as currently expressed in paragraph 3 and 4 respectively, but also about technical bias and emerging bias. [...]",,x,,x,x,,,,,x,x,,,,x,,,,x
GCSP,15:58:30,2024-03-04,0:03:06,"Thank you, Mr. Chair. All of us at GCSP are pleased to see you chairing the GGE and we wish you and your team all the very best with your important work. Mr. Chair, the GCSP welcomes the group's new mandate seeing it as a concrete step which moves the discussion forward. We firmly believe that the work of the group should remain grounded in a realistic assessment of technological realities linked to lethal autonomous weapon systems as well as the ways in which these technologies are used at present. We would therefore like to remind the group that a sober assessment of the state of autonomous weapon systems is needed. In 2024, it is high time the group began appreciating the fact that we are already today dangerously close to the full realization of laws. A recent Russian study found that the software, hardware and expertise necessary to develop a minimally functioning laws are widely available. Evidence from the battlefields of Ukraine and Gaza further shows us the reality of an already highly autonomous battlefield where decision making, targeting decisions and even target engagement are already highly algorithmically assisted with varying levels of actual human oversight and involvement. These active war zones are acting as a catalyst for the development and deployment of these weapons. The GCSP urges the group to more directly use evidence from the real world use of these autonomous capabilities in its work. In light of the group's convergence towards the need for some level of control in the use of autonomous weapons to guarantee their compliance with IHL and in view of discussing elements of an instrument, the GCSP encourages states to use the 2024 GGE to further clarify human control requirements for the appropriate use of autonomous weapon systems and consider making these clarifications part of any instrument resulting from the group's work. Specifications are direly needed, less states resort to so-called instances of nominal human control. Mr. Chair, research into automation bias clearly shows that humans have a tendency to increasingly defer to machine suggestions the more autonomous a system is. Increasing autonomous weapon systems might therefore result in increasingly performative or nominal human control measures. Failure to clarify what amounts to meaningful human control which actually ensures potential compliance with IHL will result in a situation where weapon systems are developed with various kinds of human oversight mechanisms, meaningful, effective or appropriate only in name. This would not be a satisfactory level of human involvement. Here human oversight would legitimize these weapon systems in the eyes of IHL without acting as an actual effective fail safe. Regardless of the qualifier it should refer to control in which a human has the necessary contextual understanding and cognitive and physical capacity to critically engage with any systems, suggestions or actions. Our full statement will be available on the website. Thank you, Mr. Chair.","[...] Mr. Chair, research into automation bias clearly shows that humans have a tendency to increasingly defer to machine suggestions the more autonomous a system is. Increasing autonomous weapon systems might therefore result in increasingly performative or nominal human control measures. [...] ",,,,x,x,,,,,,,,,,,,,,
RUSSIAN FEDERATION,11:00:15,2024-08-30,0:02:40,"Thank you, Mr. Chairperson. I hope that our comments will encourage other Delegations to launch the interactive discussion. On this section, we have comments on Paragraph 1 here we would prefer after rigorous testing to add including in real operation environment. To repeat. Including in real operation environment. We believe it is very important to add that. It therefore gives the testing carried out by a state on these weapons systems towards not only virtual regimes, but also and above all, which is the most valuable thing in determining the specific characteristics and particularities of operation of a system through testing in real operational conditions. In real operation environment. We would also have comments on the last two paragraphs, namely 3 and 4. These we believe are interconnected and somewhat duplicate one another. So as we see it, there is scope to combine them. If we take 4 as our basis, then it could read as follows in English. States should implement measures to detect and reduce automation bias. And that would make the wording reasonably broad. It would look quite flexible. And it would make it possible for states, parties to work on resolving this issue on a large scale. I thank you.","[...] We would also have comments on the last two paragraphs, namely 3 and 4. These we believe are interconnected and somewhat duplicate one another. So as we see it, there is scope to combine them. If we take 4 as our basis, then it could read as follows in English. States should implement measures to detect and reduce automation bias. And that would make the wording reasonably broad. It would look quite flexible. And it would make it possible for states, parties to work on resolving this issue on a large scale. [...]",,,,x,x,,,,,,,,,,,,,,
UNITED STATES,12:06:38,2024-08-30,0:10:42,"I recognize the Distinguished Representative of the United States, you have the floor. I think first we just wanted to appreciate the very rich and constructive discussion the group is engaged in. We prepared thoughts reacting to your text but we are also very interested in considering carefully the comments and proposals from other Delegations. As you and others can imagine, some of the issues here can be very technical and we will need to consult more with experts, further experts and capitals on these issues but we are very eager to do that and we are glad that we are at this point in the discussions that we are advancing our work in such a rigorous and methodical way. Mr. Chair, my Delegation wanted to recall our previous suggestion to merge the measures in this section with the prior section, that includes limitations on the types of target, duration of the system, et cetera. We see these measures as fundamentally similar. All of them are supporting compliance with IHL. I also wanted to note as my Delegation mentioned yesterday that we would appreciate clarifying the connection between these measures and particular rules of IHL. So that is a thematic comment. Just being as specific as possible, we do suggest changing the title of the section from measures to ensure compliance to measures to support compliance. For example, in our view, testing and evaluation is an important measure that helps support compliance but it cannot ensure compliance. I think we also listened with interest to our Israeli colleagues' suggestion of advanced compliance and this could be a very good solution. My Delegation also wanted to react to some of the other I think more general elements that people have been discussing. We heard ideas like appropriate control being mentioned by Israeli colleagues or context appropriate human involvement that UK colleagues had mentioned. I think our perspective had generally been that in developing the elements of a potential instrument we need to be as specific and clear as possible. But I think we also appreciate where other Delegations are coming from, that they want to talk in more general terms. And one idea we would offer to the group to start to bridge some of these gaps is to recall guiding principle C which was developed after very lengthy negotiations in this group and after some I think terrific work by the Belgian Delegation in particular in leading our efforts on that guiding principle. And the basic idea in guiding principle C is that human machine interaction which may take various forms and be implemented at various stages of the life cycle of a weapon should ensure that the potential use of laws is in compliance with applicable international law in particular IHL in determining the quality and extent of human machine interaction a range of factors should be considered including the operational context and the characteristics and capabilities of the weapon system as a whole. And I think there is a lot of good language and ideas in guiding principle C and I think our suggestion is thinking about ways to elaborate and build upon guiding principle C rather than starting fresh. My Delegation also wanted to express our support for adding in this section guiding principle E which was also adopted in our 2023 report in accordance with state's obligations under international law in the study development acquisition. I will not repeat the whole thing but do want to register our support for adding that language in to our draft elements of an instrument. Moving to the specific paragraphs you proposed regarding paragraph 1 I think we would concur with the concern that UK colleagues have noted that the reference to operator in paragraph 1 May be overly narrow because there can be a range of individuals who should have this reliable expectation who need to know about the capabilities of the weapon when they are planning or authorizing the use of the weapon system. I think our suggestion was to omit it here but we are open to considering other proposals. We also wanted to react to the proposal from France to replace reliable expectation with appropriate knowledge and here we think this is a very complex issue and we will need to think carefully about this. In our view appropriate is less clear and more subjective than reliable. And I would also note we listened to our Indian colleagues who thought that reliable is itself too subjective. So I think maybe this is an area for further discussion and work and consideration. Reacting also to French colleagues' suggestion to replace expectation with knowledge, I think in our view knowledge is not quite right because it lacks this idea of the future. We know things that happened in the past but testing and evaluation is generating an assessment or an expectation of what may occur in the future. So again really just appreciate the very thoughtful consideration that colleagues are giving and look forward to thinking about this more. Regarding Paragraph 2, we appreciate the inclusion of this paragraph but do recommend some refinements. In our view the purpose of a legal review is not to understand the weapon's capabilities and limitations, rather the legal review is considering those capabilities and limitations in the context of international law. So our suggestion is that it read before using a laws, states should conduct a legal review of the system that includes consideration of the weapon's capabilities and then the text continues as written. And then I would also note we were trying to add the idea that the legal review occurs before the use. Of course it is not before every use that there is an individual legal review but before deploying the system, states need to have conducted a legal review to ensure that it can be used lawfully. With regard to Paragraph 3, again a very rich discussion here. We wanted to agree with a number of colleagues on this aspect. In our view the purpose of the review is to address unintended bias that will result in bad outcomes rather than just to I think detect bias per se. So we wanted to connect this paragraph like other paragraphs to the goals, the objectives and purposes of the CCW and of IHL such as the protection of civilians. Our proposal would be states should implement measures to address possible unintended bias in artificial intelligence capabilities relied upon in connection with the use of the laws that increase the risk of the loss of civilian life, injury to civilians or damage to civilian objects. We appreciate a divergence of views in the room as to whether bias here is inherently problematic or bad or illegal. From our perspective we understand bias in AI capabilities to be a very technical matter. We do not think this is necessarily inherently bad or unlawful. So in our policies I think we refer to the concept of unintended bias which we think is important to include here. Regarding the next paragraph, we did want to agree with what UK colleagues mentioned about automation bias. So I will mention that we do see automation bias as something that as such is an inherently problematic matter and therefore we would delete unwanted. And I think the last general reflection I would note is we appreciate hearing these different perspectives on terminology. Often sometimes it is the UK and the US which are people which are separated by a common language. But I think trying to develop common understandings and definitions, common understandings of the same term is exactly the kind of work that we need to do and are doing. So we just wanted to appreciate, Mr. Chair, your leadership and colleagues willingness to engage in this work. I thank you, Mr. Chair.","[...] With regard to Paragraph 3, again a very rich discussion here. We wanted to agree with a number of colleagues on this aspect. In our view the purpose of the review is to address unintended bias that will result in bad outcomes rather than just to I think detect bias per se. So we wanted to connect this paragraph like other paragraphs to the goals, the objectives and purposes of the CCW and of IHL such as the protection of civilians. Our proposal would be states should implement measures to address possible unintended bias in artificial intelligence capabilities relied upon in connection with the use of the laws that increase the risk of the loss of civilian life, injury to civilians or damage to civilian objects. We appreciate a divergence of views in the room as to whether bias here is inherently problematic or bad or illegal. From our perspective we understand bias in AI capabilities to be a very technical matter. We do not think this is necessarily inherently bad or unlawful. So in our policies I think we refer to the concept of unintended bias which we think is important to include here. Regarding the next paragraph, we did want to agree with what UK colleagues mentioned about automation bias. So I will mention that we do see automation bias as something that as such is an inherently problematic matter and therefore we would delete unwanted. [...]",,,,x,x,,,,,,,,,,,,,,
AUSTRALIA,12:25:03,2024-08-30,0:03:53,"Thank you for the opportunity to discuss other measures to ensure compliance with international law. We are listening closely to the very useful suggestions that have been made this morning. As noted in our intervention yesterday, we see value in delineating clearly what the group understands to be legally binding obligations from those specific practical measures in the context of autonomous weapon systems that can help states give effect to their legally binding obligations. In this respect, if the box is to retain its current title, other measures to ensure compliance with international law, we see value in the drafting suggestions made by the Delegations of Israel and the US Looking more closely at the paragraph on legal reviews, the second paragraph, we are pleased to see this included in the rolling text for those states who conduct legal reviews including pursuant to additional protocol 1 of the Geneva Conventions. This is an important obligation and inclusion in this text. Australia has long advocated for the importance of legal reviews as a mechanism for ensuring that the development of use of autonomous weapon systems in armed conflict is lawful under armed conflict. This is what is expressed in our Working Paper, Working Paper No. 9 which is submitted to you, Mr. Chair, on the legal review of autonomous weapon systems and the outcomes of the recent expert meetings held in Australia. We support Brazil and Canada's suggestions to include a reference to the exchange of best practice in this paragraph. We do see some language in the current drafting that does, however, go beyond what is required by Article 36 of additional protocol 1. We have heard Austria, Brazil, France, Ireland, Sweden, Switzerland and United Kingdom call for language in the paragraph that either more closely aligns with the consensus language from the guiding principles and the 2023 report or to Article 36 of additional protocol 1. There are two suggestions that we might make that flow from this. Firstly, to include an appropriate qualifier, for example, in accordance with the state's obligations under international law, to reflect the fact that some high contracting parties but not all have these obligations pursuant to Article 36 of the first additional protocol. Secondly, ensure language used is clear and that newly drafted text reflects IHL obligations as opposed to being a regulatory measure. We have heard with interest the ICRC's proposal to separate legal reviews from technical reviews and we think this is a neat solution. Finally, on paragraphs 3 and 4, we wonder whether those two paragraphs could be combined so states could implement measures to detect and reduce automation bias. Thank you, Chair.","[...] Finally, on paragraphs 3 and 4, we wonder whether those two paragraphs could be combined so states could implement measures to detect and reduce automation bias. [...]",,,,x,x,,,,,,,,,,,,,,
GERMANY,11:54:55,2024-03-04,0:03:40,"First of all, I would like to join other than congratulating you on taking over the Chairmanship of this important group. You have the full support of my Delegation. Germany welcomes that the GG on laws will now start to work under its new mandate. The mandate to further consider and formulate by consensus a set of elements of an instrument gives us the opportunity to continue the valuable substantive discussions of this group in a more focused way. In our view, the substantive basis for this assignment has been laid in the past years. A report published last week states that, and I quote, it is increasingly clear that the majority of states agree on certain key elements. We think so, too. In the course of the past years and despite the deplorable blockade of the GGE to consent the final report in some of those years, the areas of substantial convergence have been growing. First of all, more and more members of the GGE have the view that prohibitions and regulations are needed to ensure that international law, including international humanitarian law, will be maintained when emerging technologies in the area of lethal autonomous weapons systems are being used or developed. Second, we also see broad agreement on the importance of the role of humans in the context of autonomy. There has even been some movement with regard to the most difficult question of the legal nature of an instrument. A growing number of states would by now accept it if the GGE were to consent a legally binding instrument consented within the GGE in order to prohibit fully autonomous weapons. Germany is one of them. We also see room for intensified discussions how to avoid risks and unintended consequences. This lies in the interest of all of us. We, therefore, welcome your decision to focus the upcoming GGE's discussion on risk mitigation and confidence building measures. We highly welcome the focused approach you have chosen, Mr. Chair, and stand ready to do all we can to support you and your efforts. With regard to the working mode of the group under the new mandate, Germany would see great benefit in starting to work on possible text elements as soon as possible. In our view, the group should aim at consenting written elements of an instrument, preferably before the CCE revcon in 2026. This could finally demonstrate that the GGE is capable not only of very good discussions but that it also is able to deliver. Germany continues to highly value the GGE as a forum for the discussions on laws that includes all relevant stakeholders. We are very happy to be among the cosponsors of the working paper on elements of an instrument based on the two-tier approach that has been introduced by France earlier this morning. We hope it will provide helpful input on how possible elements of an instrument could look like. To conclude, allow me also to inform Delegations and observers about a side event on fixing gender glitches in military AI, mitigating unintended biases and tackling risks which we organized with the support of Canada, Costa Rica, Panama, Mexico. It will take place on Wednesday, 6 March during lunch break. I hope for fruitful discussion this week, discussions that can also have an impact on the national contributions for the report by the UN Secretary General. I thank you, Mr. Chair.","[...] To conclude, allow me also to inform Delegations and observers about a side event on fixing gender glitches in military AI, mitigating unintended biases and tackling risks which we organized with the support of Canada, Costa Rica, Panama, Mexico. It will take place on Wednesday, 6 March during lunch break. [...] ",,x,,x,,,,,,,,,,,,,,,
INTERNATIONAL COMMITTEE FOR ROBOT ARMS CONTROL,17:24:14,2024-03-06,0:02:02,"Once again, thank you, Chair. My apologies for taking the floor again. I want to start by expressing appreciation to the US Delegation for their response. The first thing which we want to emphasize again is the question that consensus language in the UNGG reports is important but must not be inconsistent with existing IHL language or other obligations. In relation first to the aspect of unintended engagement which the Delegation has explained that this refers to the system engaging targets other than those authorized by the operator, does this mean then that the system has autonomy in critical functions? Does this not fall under autonomous open systems that must be prohibited rather than mitigated? In relation to the second point on unintended bias, what again is meant by this? Chair, in terms of rule 88 of customary international humanitarian law, the words that are used is adverse distinction. In terms of international human rights law treaties to which Delegates or States are party, the term that is used is the right to non-discrimination. In our view, the risk here can be properly identified as risk to the right to non-discrimination. The word unintended is not helpful because gender and racial discrimination or any other form of discrimination is not only harmful because it is intended. It is harmful because it is discrimination. And as ICRC, we insist on using language that already exists in binding treaties. Thank you.","[...] In relation to the second point on unintended bias, what again is meant by this? Chair, in terms of rule 88 of customary international humanitarian law, the words that are used is adverse distinction. In terms of international human rights law treaties to which Delegates or States are party, the term that is used is the right to non-discrimination. In our view, the risk here can be properly identified as risk to the right to non-discrimination. The word unintended is not helpful because gender and racial discrimination or any other form of discrimination is not only harmful because it is intended. It is harmful because it is discrimination. And as ICRC, we insist on using language that already exists in binding treaties. [...] ",x,x,x,x,,,,,,,,,,,,,,,
DENMARK,12:18:10,2024-08-26,0:02:58,"Mr. Chair, allow me to start by thanking you for your chairmanship of the group of governmental experts on laws and express our continued support for the group's important work. At the outset, we wish to acknowledge the progress of the GGE that we have made this year. The constructive discussions during the March session and the formal consultations have resulted in growing convergence on a range of issues. This year we are discussing concrete language for elements to include in a future instrument. We hope that this can lead to a substantial breakthrough. At the same time, we also recognize the need to bridge some gaps between different national positions. We strongly believe that the CCW continues to be the relevant international forum for dealing with these issues. Mr. Chair, we recall that international law does not depend on the particular technological means employed. Choice of weapons, means and methods of warfare including lethal autonomous weapons systems must comply with international law including the IHL principles of distinction and proportionality. Denmark supports the two-tier approach indicating that a distinction should be made between those weapons systems that cannot be used in accordance with international law and those systems that includes autonomous features the use of which should be appropriately regulated in order to ensure compliance with international law. Furthermore, we are pleased to see that the two-tier approach is included in the Chair's proposal for a rolling text. For a full account of our perspectives, we refer to the working paper submitted by Bulgaria, Denmark, France, Germany, Italy, Luxembourg and Norway for the previous session in March. Denmark welcomes the recent report by the Secretariat General. We support the report's inclusion on the topic of human control which is necessary to ensure compliance with international law and especially IHL. We recall that responsibility and accountability cannot be delegated to machines. We wish to emphasize the importance of conducting legal reviews of weapons, means and methods of warfare and the implementation of risk mitigation measures to ensure compliance with international law, in particular IHL and to address relevant ethical concerns of laws. Risk mitigation measures should address the harmful effects of potential biases instilled in the weapon system through the application of artificial intelligence including biases related to gender, ethnicity, age and disability. Lastly, we wish to express our continued commitment to the group's work. Denmark is fully committed to deliver and fulfill the mandate of the group as soon as possible, preferably before the end of 2025. Therefore, we urge all countries to continue the constructive efforts in reaching a substantial outcome this year for the benefit of us all and to enable us to continue this important work. Thank you, Mr. Chair.","[...] Risk mitigation measures should address the harmful effects of potential biases instilled in the weapon system through the application of artificial intelligence including biases related to gender, ethnicity, age and disability. [...]",,x,,x,,,,,,,,,,,,,,,
THE CIVILIAN AGENDA,15:23:34,2024-08-26,0:03:19,"Autonomous weapon systems have already proliferated and are being deployed in contemporary conflicts to commit war crimes. The range of serious legal, ethical security and humanitarian concerns are now a lived reality. As such, states at the CCW have failed in their duty to safeguard against the grave dangers posed by these weapons. We are now required to mitigate the harm already done beginning with an urgent moratorium on the development, transfer and use of autonomous weapon systems. While CCW states have deliberated for 11 years, militarized states have now deployed autonomous weapons and civilians have been killed. These systems are capable of analyzing vast amount of human data, recognizing our faces, evaluating our relationships and opinions and making life or death decisions based on an algorithm. Autonomous weapon systems are no longer a theoretical concern. Killer robots have arrived. As reported by Ashwini KP, UN special rapporteur on racism in July, there is, and I quote, serious risk of grave and in some circumstances deadly racial discrimination resulting from the use of autonomous weapon systems, end quote. Noting that autonomous weapon systems have, and I quote again, intensified the levels of destruction in Gaza resulting in significant casualties, in particular among Palestinian women and children. I end quote. But it is not just Gaza. These weapons can be developed and used by anyone. No state or entity will have a monopoly over them. The further proliferation of these weapons threaten international peace and security and the moratorium is urgently required to mitigate the damage. We are pleased to hear that an interfaith event will be held this week on this issue. While a broad range of risks have been evaluated, the most foundational threat posed by autonomous weapon systems is to our shared humanity. To overcome these challenges, humans must recognize the importance of protecting the values that make our human experience possible. Listening to faith leaders on this profound topic is crucial to our progress. Chair, we commend the draft text that you have circulated and believe it provides a solid foundation for the work for this week. In particular, we appreciate the clarification on nominal human input closing a significant loophole which has so far allowed states to develop and use autonomous weapon systems but avoid calling it as such to prevent scrutiny. We also agree with the separation between prohibition and regulations and we stress the necessity for an explicit prohibition on autonomous weapon systems that target human directly. We now need decisive and collective action to issue a moratorium and agree on a legally binding instrument to protect our shared humanity, safeguard the principles that bind us together and prevent the imminent and unrestrained proliferation of autonomous weapons around the world. Thank you, Chair.","[...] These systems are capable of analyzing vast amount of human data, recognizing our faces, evaluating our relationships and opinions and making life or death decisions based on an algorithm. Autonomous weapon systems are no longer a theoretical concern. Killer robots have arrived. As reported by Ashwini KP, UN special rapporteur on racism in July, there is, and I quote, serious risk of grave and in some circumstances deadly racial discrimination resulting from the use of autonomous weapon systems, end quote. [...]
",x,,x,,,,,,,x,x,,,,,,,,
GERMANY,11:47:08,2024-08-30,0:01:06,"Thank you, Mr. Chair. Our Delegation has studied with interest the working paper submitted by the Distinguished Delegation of Brazil on possible uses of the IEEE standard in the context of the work of this group. We would welcome our group to look further into the suggestions made. In Germany's view, it remains very important that our group takes challenges posed by unintended biases on aspects such as on gender, race or disability into account. Some relevant aspects in this regard have been outlined in a working paper submitted by Austria, Belgium, Canada, Costa Rica, Germany, Ireland, Luxembourg, Mexico, Panama and Uruguay in March. In this regard, we would also like to welcome academic efforts and activities such as publishing a very helpful fact sheet this week. The valuable discussion at the side event organized by UNIDEA this week demonstrated the importance to continue our efforts. As mentioned by some Distinguished Colleagues before, we would very much welcome further substantive discussions on this issue. Thank you.","[...] In Germany's view, it remains very important that our group takes challenges posed by unintended biases on aspects such as on gender, race or disability into account. Some relevant aspects in this regard have been outlined in a working paper submitted by Austria, Belgium, Canada, Costa Rica, Germany, Ireland, Luxembourg, Mexico, Panama and Uruguay in March. In this regard, we would also like to welcome academic efforts and activities such as publishing a very helpful fact sheet this week. The valuable discussion at the side event organized by UNIDEA this week demonstrated the importance to continue our efforts. As mentioned by some Distinguished Colleagues before, we would very much welcome further substantive discussions on this issue. [...]
",x,x,,x,,,,,,,,,,,,,,,
CUBA,16:15:40,2024-03-05,0:03:40,"Thank you, Chair. To address your questions under the application of IHL including the concept of human control, judgment or involvement, we would like to point out the following. For our Delegation we cannot limit the scope of the discussions and curtail it only to IHL. Autonomous weapon systems and semi -autonomous weapon systems must be considered in the context of the observance of international law which means that all and any autonomous weapon that does not meet the provisions of international law including IHL must be banned even before they begin to be developed and used in large scale and to be deployed. This is a preventive approach based on the principle of precaution. Systems that do not guarantee significant human oversight nor do predictability, trustability, traceability, reliability and cannot be explained cannot be developed either. Sir, the use of completely autonomous weapons raises challenges for the fulfillment and the observance of the standards and principles of international law that cannot be guaranteed as these weapons could be used counter to the principles of sovereignty and territorial integrity of states which are enshrined in Article 2 of the United Nations charter. The principle of distinction under IHL demands that human beings that resort to force draw a distinction between those participating in the hostilities and that as such can be attacked and all others including civilians, combatants, persons or to combat and other protected persons with laws this principle cannot be insured. These weapons systems can perpetuate and amplify biases including those that are racial or gender biases depending on the data set that they use as a foundation. As we begin our discussion with a view to highlighting the various elements of an instrument we would like to reiterate our concern over the use of these weapon systems and the dehumanization of conflicts. In our view machines cannot replace the human beings in the most important decisions of war. There must be a prohibition of all those that do not have human control over these aspects. The degree of autonomy and lethality are basic characteristics that ought to guide the prohibition or regulation of autonomous weapons. Qualitative judgments which can only be made by humans are of key importance in the terrain of military operations as they provide greater guarantees and fulfillment of international law and they guarantee the individual responsibility as well as states' responsibilities. For our Delegation the laws that cause unnecessary suffering have indiscriminate effects and can produce generalized long lasting and serious harm to the environment must be prohibited and as such they are not compatible with current IHL. Thank you.",[...] These weapons systems can perpetuate and amplify biases including those that are racial or gender biases depending on the data set that they use as a foundation. As we begin our discussion with a view to highlighting the various elements of an instrument we would like to reiterate our concern over the use of these weapon systems and the dehumanization of conflicts. In our view machines cannot replace the human beings in the most important decisions of war. [...],x,x,,x,,,,,,x,,,,,,,,,
SWITZERLAND,16:21:39,2024-03-06,0:05:54,"Thank you, Mr. Chair. We really appreciate this discussion on risks and we broadly agree with the topics that you have listed. We are moving very fast and what we are going to contribute at this stage is just the first reaction. We might come back with more specific points later. I also want to say that it is hard to follow the Distinguished Colleague of Pakistan who spoke just before me because a number of the points that I am going to say he just made them very eloquently. Our overall understanding on the broader topic of risk mitigation confidence building measures is that in addition to measures that are strictly necessary to ensure compliance we also need additional measures, complementary. Our view is also that such measures must not be in the vacuum. They must not stand isolated. They must not stand instead of a clear regulation. One way to see this is that we perhaps would anticipate a further future legal instruments with prohibitions and regulations and then a complementary set of practices, measures, for instance, outlined in an annex to a protocol or to an instrument of whatever nature it is. We think we can now start thinking a little bit more clearly about how this could articulate the provisions, the legal provisions and then further measures. At this stage I would first like to pick up the discussion about the risks overall and like other speakers before me, try to talk a little bit about what we have here on the screen. Our first general point and here as Pakistan has just said we think that this list could be streamlined, structured, categorized. We have talked about risks in the past and we were able to line them up in a coherent fashion. Then a second very important point relates to what Austria and Mexico have said before us. Risks must be really better understood. There are many possible risks that we are just about to understand here. We might not yet have the full picture, even risks, there might even be risks that today are yet unknown. This is really a difficult discipline here for us because the technologies are emerging and so it will be very important that we start thinking about how we will be able to continuously improving our understanding of risks in a comprehensive manner. And that comprehensive manner, that brings me to another general point. It is important for us that we in this forum where we pursue an IHL focus stay aware of the fact that while IHL compliance is extremely important and noncompliance is, of course, an enormous risk as these weapons develop, there is also other risks and concerns. Risks that go beyond of what we are looking at usually in this forum. For instance, ethical aspects, they have been mentioned by others, but then also potential impact on international security, stability, for example, the potential possibility of an arms race, escalation risks, decrease in crisis stability, and then we have more technical risks, guiding principle F in 2019 already referred to physical security, hacking, data spoofing, et cetera. Then we have also talked a lot about the unintended biases related to gender, race, age, ability, et cetera. And we have also referred in the past to the risk of acquisition by terrorist groups, nonstate actors and the risk of proliferation. And to end this first intervention, I would just briefly remind Delegates of the preambular paragraph 4 of the UNGA resolution adopted last year which kind of made a good summary of the concerns that are out there, perhaps also out there beyond the CCW and which are on the top of the minds of 177 countries, I believe, and that is concerns about global security, regional and international stability, including the risks of an emerging arms race, lowering the threshold for conflict and proliferation, including to nonstate actors. So it is not to say that the risks that are on the screen here are not the right ones to list, but it is just to make sure we stay on top of things and have a broader risk perspective and that we aim to always try to better understand risks that are only about to emerge. Thank you.","[...] Risks that go beyond of what we are looking at usually in this forum. For instance, ethical aspects, they have been mentioned by others, but then also potential impact on international security, stability, for example, the potential possibility of an arms race, escalation risks, decrease in crisis stability, and then we have more technical risks, guiding principle F in 2019 already referred to physical security, hacking, data spoofing, et cetera. Then we have also talked a lot about the unintended biases related to gender, race, age, ability, et cetera. [...] ",x,x,,x,,,,,,x,,,,,,,,,
PANAMA,11:24:48,2024-08-30,0:04:07,"Thank you very much, Chairman. As to this section, I would like to start out by supporting the proposal made by the ICRC to have an additional paragraph. I understand that would be a type of a zero paragraph and now on states' obligations when it comes to the study acquisition and adoption of any law systems determining whether their use in some circumstances would violate IHL, we believe that this is a fundamental addition to this segment. As regards paragraph 1, as such, we think that comments made by the Russian Federation and Brazil could be incorporated. On the second paragraph, we support the comments made by the ICRC, Austria and Switzerland. We believe that the current wording is quite weak bearing in mind that this is a legal obligation enshrined in additional protocol 1. I would also recall and some states, however, argue who are not states' parties to this instrument, the vast majority of their provisions constitute customary international law including Article 36. That is why we would support its wording to abide by it. On paragraphs 3 and 4, we would like to recall that pursuant to the proposal protocol 6 which was presented by the G16 during the sessions last year, we believe that the adoption of measures and mechanisms to avoid algorithmic biases should be part of regulations. In that regard, we would invite you to consider the proposal set out in that document. This is Article 4, section 2. At the same time, we would like to recall that a group of countries including Austria, Belgium, Canada, Costa Rica, Germany, Ireland, Luxembourg, Mexico, Uruguay and my own country, Panama submitted a working document this year to address bias in autonomous systems which includes a set of measures to mitigate its effect. We would also like to ask you to consider those aspects which are reflected in paragraph 11 of the document. We would like to echo Delegations of Austria, I believe, and others as well as civil society in that we would not agree with maintaining qualifiers referring to bias because bias already have a negative connotation. It would give the impression that there are bias that are permissible. Many of these are based on race and precisely the prohibition of racial discrimination takes on illegal status of a norm of international law and obligations. That is why we need to be very cautious when it comes to these type of wordings. We do not want to give the impression that there are bias that are positive or permissible, particularly within the context of weapons which will have a certain level of autonomy in the selection process of their targets. Thank you, sir. Yes.","[...] On paragraphs 3 and 4, we would like to recall that pursuant to the proposal protocol 6 which was presented by the G16 during the sessions last year, we believe that the adoption of measures and mechanisms to avoid algorithmic biases should be part of regulations. In that regard, we would invite you to consider the proposal set out in that document. This is Article 4, section 2. At the same time, we would like to recall that a group of countries including Austria, Belgium, Canada, Costa Rica, Germany, Ireland, Luxembourg, Mexico, Uruguay and my own country, Panama submitted a working document this year to address bias in autonomous systems which includes a set of measures to mitigate its effect. We would also like to ask you to consider those aspects which are reflected in paragraph 11 of the document. We would like to echo Delegations of Austria, I believe, and others as well as civil society in that we would not agree with maintaining qualifiers referring to bias because bias already have a negative connotation. It would give the impression that there are bias that are permissible. Many of these are based on race and precisely the prohibition of racial discrimination takes on illegal status of a norm of international law and obligations. That is why we need to be very cautious when it comes to these type of wordings. We do not want to give the impression that there are bias that are positive or permissible, particularly within the context of weapons which will have a certain level of autonomy in the selection process of their targets. [...]",x,,x,x,,,,,,,x,,,,,,,x,x
ARGENTINA,11:51:20,2024-08-30,0:02:08,"Thank you, Chairperson. Argentina shares the concept that strengthens the reliable or appropriate knowledge as indicated by France for the use of lethal autonomous weapon systems. There are controls on production and the broadening of responsibility of manufacturers. If that is the case, we still need to step up control or prohibition of products that are produced without supervision in order to avoid black boxes in software. Argentina sees the need to limit the use of algorithms or systems that are programmed which seek to identify based on race, gender or any other discriminatory identification countered to international law including IHL. It is suggested that we should consider the possibility of including a paragraph that would cover ways to prevent proliferation of technology that does not meet standards for control and autonomy with human control. And finally, we support Brazil's proposal on the reference to realistic simulated scenarios. Furthermore, we have doubts about the unwanted bias and support was said by Panama Island and other Delegations on the need to revise the text both in terms of the objectives and the modal verb should in order to make these paragraphs consistent with imperative norms of international law. Thank you.","[...] Argentina sees the need to limit the use of algorithms or systems that are programmed which seek to identify based on race, gender or any other discriminatory identification countered to international law including IHL. [...]
Furthermore, we have doubts about the unwanted bias and support was said by Panama Island and other Delegations on the need to revise the text both in terms of the objectives and the modal verb should in order to make these paragraphs consistent with imperative norms of international law. [...]",x,x,x,x,,,,,,,x,x,,,,,,,
AUSTRIA,17:36:48,2024-03-06,0:02:26,"I think everyone has deserved to go home at this moment but it is a very interesting topic and I think it is a concept that deserves a little bit more attention. The big question here is the question is there unintended bias or not? It is a conceptual question, probably one we should agree. There are different ways of seeing it. We see it and this comes probably also from the kind of work that has been done in other international organizations on AI as something that is per se unintended. The problem if you talk about bias is that bias is a reflection of something that is already intrinsic in societies. If you talk about bias in our context, any kind of discrimination that is there to comply with IHL is not bias. It is something that has been deliberately programmed into the system to comply with IHL. In that case it is not bias because bias would more or less be an accident. We had this discussion today. There are different types of biases. There is the kind of bias that stems from faulty datasets or not programming but kind of the way the datasets are selected, implemented, maintained. Also the cognitive biases that exist in human existence or the way that the human brain works. I have here a list that includes stereotyping, bandwagon effect, priming, selective perception or confirmation bias. Those are just cognitive biases that you can see in everyday's world and that are then introduced into an algorithmic system and then you have something that has come up often today, real life prejudices which are racial bias, gender bias and so on. It is a very wide area. It is all in most cases faulty. It is not intentional and a bias that is there for having kind of a positive discrimination on the battlefield in our interpretation is not a bias but it is something that has been placed there to comply with IHL. Thank you.","[...]The big question here is the question is there unintended bias or not? It is a conceptual question, probably one we should agree. There are different ways of seeing it. We see it and this comes probably also from the kind of work that has been done in other international organizations on AI as something that is per se unintended. The problem if you talk about bias is that bias is a reflection of something that is already intrinsic in societies. If you talk about bias in our context, any kind of discrimination that is there to comply with IHL is not bias. It is something that has been deliberately programmed into the system to comply with IHL. In that case it is not bias because bias would more or less be an accident. We had this discussion today. There are different types of biases. There is the kind of bias that stems from faulty datasets or not programming but kind of the way the datasets are selected, implemented, maintained. Also the cognitive biases that exist in human existence or the way that the human brain works. I have here a list that includes stereotyping, bandwagon effect, priming, selective perception or confirmation bias. Those are just cognitive biases that you can see in everyday's world and that are then introduced into an algorithmic system and then you have something that has come up often today, real life prejudices which are racial bias, gender bias and so on. It is a very wide area. It is all in most cases faulty. It is not intentional and a bias that is there for having kind of a positive discrimination on the battlefield in our interpretation is not a bias but it is something that has been placed there to comply with IHL. [...]",x,x,x,x,,x,,,,x,x,,,,,,,,
MEXICO,17:26:42,2024-03-05,0:07:03,"Thank you very much, Chair. And to begin, I would like to express my apologies because due to the very charged disarmament agenda I was unable to be present this morning and I arrived late to this afternoon's session. So I can not offer very interactive comments given that I was unable to listen to all Delegations unfortunately. But I will make a presentation about some of these items from our perspective. I would like to begin by mentioning the seminar report of Christopher Hines, special rapporteur on extrajudicial executions who in 2013 already, and we believe this is still quite relevant for our discussions today, said that autonomous weapon systems raise serious and far reaching concerns for the protection of people and lives in both peace and war. This includes a question about the ways in which they can be programmed in order to meet the IHL requirements as well as the protection standards of life under international human rights law. Obviously, this perspective recognizes the necessary interaction between the IHL system and international human rights law which is backed by its jurisprudence and the impossibility of creating a false division in the way in which they ought to be considered and so for our Delegation, there is no question that any weapon system must be developed, deployed and used in keeping with IHL but also with international human rights law, international criminal law and the law and international responsibility in the UN charter. Though Mexico considers that significant human control is an implicit component in IHL, we also take into account existing differences, those that we have been discussing for a decade. So we appreciate the presentation of your list, the various components and elements that would make laws incompatible with IHL. However, we believe along with ICRC and Austria and others that the goal is not simply to have a list of the prohibitions or general applicable regulations but rather to define the specific relevant aspects to ensure that its implementation is compatible bearing in mind the specific elements of autonomy. And so in that regard, we agree with the list that reiterates already existing prohibitions of IHL, particularly those that by their nature are prohibited given that they cannot be used in keeping with existing limitations. For example, being unable to distinguish between military and civilian targets, between combatants and civilian population, between active combatants and persons or to combat, those that cause superfluous damage or unnecessary harm or those that have effects that cannot be limited in keeping with the fundamental principles of IHL, specifically on grounds of distinction and proportionality. Ten years of studying this subject has also shown that these prohibitions given autonomous technologies raise some very essential difficulties bearing in mind that these determinations take place in very complicated contexts and differentiated contexts which require a fundamental human assessment. So we agree with Pakistan and others in that regard who say that we need a more subtle approach, one that would identify specific and indispensable elements in order to be able to meet these general obligations. From our standpoint, some of them include that the development and use of autonomous weapons systems that cannot be controlled, I will start again, to prohibit the use and development of autonomous systems that cannot be controlled by humans for which they are limited to cognitive and epistemological limitations in addition to being subjected to different types of bias, for example, those arising from the source of information being employed or algorithmic bias. Two, that the development and use of autonomous weapons systems whose programming allows for the removal of human control over the critical functions related to the selection of targets, the involvement and use of force, the development and use of autonomous weapons systems whose effects do not meet the criteria of predictability and whether it is explainable for the operators. Or four, the development and use of autonomous weapons systems that were not subjected to a legal assessment in keeping with the obligations under Article 36 of the additional protocol 1 in customary law. Now, the simple involvement of a human in the selection operation and the involvement in the use of force does not resolve in and of itself the compatibility risks with IHL given that it seems to not involve some fundamental components of the human component. There must be a guarantee that there are proper understanding of the limitations of their use, even spatial and temporal limitations and a decision that is taken must take place when they know the operational context through a sufficient knowledge of the situational environment. It must take the necessary precautions in carrying out operations in order to ensure that parameters of the mission do not change without human assessment and that it allows for constant oversight to guarantee intervention when needed, particularly in order to interrupt and deactivate the system during the operational stage. And lastly, that the guarantee that human involvement be substantive, not just in name only. Now, all of these are elements that as has been said will require further consideration in future debates. And lastly, we would like to reiterate as we said yesterday when we agree with the ICRC and others on the relevance of the ethical perspective, deontologically speaking and under the criterion of human dignity, which is part and parcel inherently implicit in IHL and international human rights law. Thank you.","[...] And so in that regard, we agree with the list that reiterates already existing prohibitions of IHL, particularly those that by their nature are prohibited given that they cannot be used in keeping with existing limitations. For example, being unable to distinguish between military and civilian targets, between combatants and civilian population, between active combatants and persons or to combat, those that cause superfluous damage or unnecessary harm or those that have effects that cannot be limited in keeping with the fundamental principles of IHL, specifically on grounds of distinction and proportionality. Ten years of studying this subject has also shown that these prohibitions given autonomous technologies raise some very essential difficulties bearing in mind that these determinations take place in very complicated contexts and differentiated contexts which require a fundamental human assessment. So we agree with Pakistan and others in that regard who say that we need a more subtle approach, one that would identify specific and indispensable elements in order to be able to meet these general obligations. From our standpoint, some of them include that the development and use of autonomous weapons systems that cannot be controlled, I will start again, to prohibit the use and development of autonomous systems that cannot be controlled by humans for which they are limited to cognitive and epistemological limitations in addition to being subjected to different types of bias, for example, those arising from the source of information being employed or algorithmic bias. [...]
",,,,x,,,,,,,x,,x,,,,,,
INDIA,10:31:41,2024-03-07,0:03:45,"Our Delegation would like to address the issues of risk mitigation with regard to lethal autonomous weapons systems and the consequential risks with their development and use. Risk mitigation is a proactive strategy to identify, assess and mitigate potential threats or uncertainties to ensure that we tackle the risks with actionable steps. As technology continues to evolve, the international community will be confronted with the legal, operational and humanitarian challenges posed by the development and use of laws and the associated risks. The challenges necessitate a concerted effort to identify and implement effective risk mitigation measures to ensure that it complies with the principles of international humanitarian law and does not use cause unnecessary suffering or superfluous injuries. We agree that the commitments made by the United States with regard to risk assessment and associated mitigation with the risk could be grouped under the following heads. They were covered extensively yesterday. Unintended engagements, loss of control over a system, loss of human control, lack of human judgment, difficulty in assigning responsibility and these kind of issues could be grouped under an operational risk. Unpredictable system behavior, unpredictability, unintended inappropriate behavior of a system, reliability, algorithmic biases, AI biases, limited data selection, et cetera could be grouped under a technological risk. Deliberate manipulation by malicious actors, development, deployment and use by non-state actors, proliferation and acquisition of technology by terrorist groups, acquisition by unauthorized users could be grouped under proliferation risks. Other risks are protected by the provisions of IHL which requires for the operators to be trained extensively through capacity building and awareness programs in the manner which these weapons systems are utilized. This would ensure that the provisions of distinction, proportionality and precautionary attack are adhered to and targeting is affected only against specific military targets. Associated with these are confidence building measures and sharing of best practices that can be effectively worked out through international cooperation and diplomacy. We believe that in this area, civil societies and governmental organizations, non-governmental organizations could play an active role to work closely with the states' parties to evolve risk mitigation measures that would ensure that only legitimate targeting within the confines of IHL is undertaken through use of laws. Our Delegation underscores that risk is inherent to any environment, especially so when we talk about weapons and targeting and hence the importance of implementing robust risk mitigation measures to address the challenges posed by laws. We believe that these risks could be mitigated through already established practices which have been brought out by several Delegations, which could be realigned to suit the development and use of laws. In conclusion, we believe that by implementing the concrete risk mitigation strategies, laws can be developed and used in a manner consistent with international law. Thank you, Mr. President.","[...] Unintended engagements, loss of control over a system, loss of human control, lack of human judgment, difficulty in assigning responsibility and these kind of issues could be grouped under an operational risk. Unpredictable system behavior, unpredictability, unintended inappropriate behavior of a system, reliability, algorithmic biases, AI biases, limited data selection, et cetera could be grouped under a technological risk. Deliberate manipulation by malicious actors, development, deployment and use by non-state actors, proliferation and acquisition of technology by terrorist groups, acquisition by unauthorized users could be grouped under proliferation risks.[...]",,,,x,,,x,,x,x,x,,x,,,,,,
IRELAND,11:05:10,2024-03-07,0:03:16,"Thank you, Chair. Throughout the process of the GGE on laws, a range of states, international organizations, civil society, academia and industry have consistently acknowledged the risks related to artificial intelligence and bias, in particular with regard to gender and racial bias. This is something which was also raised by several Delegations during our session yesterday afternoon and was likewise the subject of a side event hosted yesterday by UNIDIR, together with the permanent missions of Canada, Costa Rica, Germany, Ireland, Mexico and Panama. At yesterday's event, it was stressed that gender and other forms of biases in AI technologies go beyond technical solutions and there is a need for collective action and cooperation in this regard. As you will be aware, Chair, bias on autonomous weapons is also something that has been discussed in a range of working papers submitted to the GGE over the year. In this vein, Canada, Costa Rica, Germany, Ireland, Panama and Mexico have submitted a new working paper on the issue of autonomous weapons and bias. As our working paper notes, algorithms and related machine learning risk repeating, amplifying or contributing to unjust biases that programmers may not be aware of or that are the result of narrow data selection. Indeed, it is already acknowledged by industry and practitioners as an uncontroversial fact that human beings consciously or unconsciously encode their own biases into the programmes they write. As some experts have noted, bias is inherent in society and thus inherent in AI as well. This represents a major challenge in how we think and approach these issues both at the GGE and elsewhere and it demands substantive collective action on our part. In our working paper, we present a range of recommendations for the GGE to address some of these issues including that the GGE clearly identifies and recognizes the risk that the integration algorithmic biases into autonomous weapon poses to fundamental rights and safety. This should be done in a factual manner and the GGE should develop appropriate measures to tackle the issue. Furthermore, we recommend that the future normative framework on autonomous weapons should include a number of positive obligations and commitments with regard to bias in the algorithms used in autonomous weapons. In order to mitigate the significant risk, they pose the fundamental rights and compliance with international law. This would include comprehensive testing and reviews in order to identify and correct potential biases and rigorous documentation of the datasets used in autonomous weapons in order to enhance traceability and transparency and provide information regarding motivation, the collection process and recommended use. In conclusion, Chair, it is clear that AI technologies mirror our societies but often in problematic ways that pose notable risks if they go unchecked. We must be proactive in engaging and addressing these. We look forward to further engagement in this area during the GGE and throughout the intercessional process. The working paper has now been officially submitted and will be circulated shortly. Thank you, Chair.","[...]Throughout the process of the GGE on laws, a range of states, international organizations, civil society, academia and industry have consistently acknowledged the risks related to artificial intelligence and bias, in particular with regard to gender and racial bias. This is something which was also raised by several Delegations during our session yesterday afternoon and was likewise the subject of a side event hosted yesterday by UNIDIR, together with the permanent missions of Canada, Costa Rica, Germany, Ireland, Mexico and Panama. At yesterday's event, it was stressed that gender and other forms of biases in AI technologies go beyond technical solutions and there is a need for collective action and cooperation in this regard. As you will be aware, Chair, bias on autonomous weapons is also something that has been discussed in a range of working papers submitted to the GGE over the year. In this vein, Canada, Costa Rica, Germany, Ireland, Panama and Mexico have submitted a new working paper on the issue of autonomous weapons and bias. As our working paper notes, algorithms and related machine learning risk repeating, amplifying or contributing to unjust biases that programmers may not be aware of or that are the result of narrow data selection. Indeed, it is already acknowledged by industry and practitioners as an uncontroversial fact that human beings consciously or unconsciously encode their own biases into the programmes they write. As some experts have noted, bias is inherent in society and thus inherent in AI as well. This represents a major challenge in how we think and approach these issues both at the GGE and elsewhere and it demands substantive collective action on our part. In our working paper, we present a range of recommendations for the GGE to address some of these issues including that the GGE clearly identifies and recognizes the risk that the integration algorithmic biases into autonomous weapon poses to fundamental rights and safety. This should be done in a factual manner and the GGE should develop appropriate measures to tackle the issue. Furthermore, we recommend that the future normative framework on autonomous weapons should include a number of positive obligations and commitments with regard to bias in the algorithms used in autonomous weapons. In order to mitigate the significant risk, they pose the fundamental rights and compliance with international law. This would include comprehensive testing and reviews in order to identify and correct potential biases and rigorous documentation of the datasets used in autonomous weapons in order to enhance traceability and transparency and provide information regarding motivation, the collection process and recommended use. In conclusion, Chair, it is clear that AI technologies mirror our societies but often in problematic ways that pose notable risks if they go unchecked. We must be proactive in engaging and addressing these. We look forward to further engagement in this area during the GGE and throughout the intercessional process. The working paper has now been officially submitted and will be circulated shortly. [...]",x,x,,x,,,,,,x,x,x,x,,,,,x,x
NGO ENCODE JUSTICE,11:34:04,2024-08-29,0:03:16,"Thank you, Chair. As this is the first time addressing the room on behalf of ENCO Justice, we would first like to thank you for your continuous efforts both before and during the meeting given the focus of our organization to advocate for the human centric development of AI, there are two points we would like to bring under attention. In the third paragraph, the view is expressed that there should be a prohibition of laws that in all circumstances make the civilian population as such, individual civilians or civilian objects the object of an attack. Now, the greatest proposed advantage of using autonomous weapons and automated decision making in general, for example, limiting combatant loss of life can be achieved if and only if the potential targets are objectified and categorized. Without this classification, including objectifying and categorizing human beings, the benefits of using algorithms for the use of force are close to none. Hence, the only way in which the civilian population is not made an object of attack would be to regulate the classification of objects within the algorithmic decision making of AWS. If we know one thing about objectifying and classifying human beings, it is that it is a first step towards a crime that we should under no circumstance invite through loopholes in any future AWS instrument, and that is genocide. We have a choice here. Either we facilitate any power with bad intentions to commit genocide or we decide to carefully regulate classification with the intent of use of force. The latter would also be aligned with good practice in AI regulation, and that is to regulate the risk rather than the specific characteristics of a system. Systems change. The value of human life does not. Then secondly, the fourth paragraph refers to the required anticipation and control over the effects of an attack which in the case of automated weapon systems requires the predictability and reliability of used algorithms. Building on the on the argument posed just before on the focus of classification, it should also be noted that over time this classification automatically becomes less accurate. This is caused by what data scientists like myself would call algorithmic decay or model drift. Decay is caused by an AI algorithm self-learning based on historic data without being able to adapt itself to a changing environment or changing relationships between variables. All this technical explanation does come to a point I promise, and that is that the only way to ensure anticipated and controlled effects of AWS is by requiring periodic reassessment of the evaluation measures that undoubtedly will have to be part of any future instrument to be effective. Summarizing, we call on high contracting parties to specifically consider the meaning of these words to the algorithmic context of automated weapon systems by focusing on the classifying and objectifying aspects and include regular reassessment of evaluation measures in any future instrument. Thank you.","[...] Then secondly, the fourth paragraph refers to the required anticipation and control over the effects of an attack which in the case of automated weapon systems requires the predictability and reliability of used algorithms. Building on the argument posed just before on the focus of classification, it should also be noted that over time this classification automatically becomes less accurate. This is caused by what data scientists like myself would call algorithmic decay or model drift. Decay is caused by an AI algorithm self - learning based on historic data without being able to adapt itself to a changing environment or changing relationships between variables. All this technical explanation does come to a point I promise, and that is that the only way to ensure anticipated and controlled effects of AWS is by requiring periodic reassessment of the evaluation measures that undoubtedly will have to be part of any future instrument to be effective. Summarizing, we call on high contracting parties to specifically consider the meaning of these words to the algorithmic context of automated weapon systems by focusing on the classifying and objectifying aspects and include regular reassessment of evaluation measures in any future instrument. [...]",x,,,,x,,,,,x,x,,,,,,x,,
EUROPEAN UNION,11:38:55,2024-08-30,0:04:59,"Thank you very much, Mr. Chair. With regard to the first three paragraphs presented in this section, the EU would like to add following suggestions, specifically on legal reviews, in the study, development, acquisition or adoption of a new weapon, means or methods of warfare, determination must be made whether its employment would in some or all circumstances be prohibited by international law. Next, accountability framework should be established by states engaged in the life cycle of emerging technologies in the area of laws. Moreover, risk assessments and mitigation measures should be part of the design, development, testing and deployment cycle of emerging technologies in any weapon systems. Tailored risk mitigation measures should be adopted and implemented across the entire life cycle of the system. We are of the view that during the design, development, testing, deployment and use of lethal autonomous weapons systems, states must consider as appropriate risks such as civilian casualties and take precautions to minimize the risk of harm to civilians and civilian objects as well as other types of risks including but not limited to the risk of unintended engagements, risk of loss of control of the system, risk of diversion to unauthorized users including terrorist groups and risk of acquisition by terrorist groups and taking into account relevant ethical principles. Risk mitigation measures could include the following : First, rigorous testing and evaluation to inform an assessment of how the weapon system will perform in the anticipated circumstances of its use. Second, legal reviews and sharing of best practices. Third, sufficient understanding depending on the role and level of responsibilities of the system's way of operating, its effect and likely interaction with the environment. Four, readily understandable human machine interfaces and controls. Five, comprehensive and systematic training of personnel. Six, the activation mechanism where appropriate. Seven, establishing doctrine and procedures. Eight, circumscribing weapons use for rules of engagement. And last, assessment by the users. With regard to the bias in the algorithm used in lethal autonomous weapon systems, the EU recognizes the critical role that data plays for AI based technologies. Social biases that could have a potential impact on emerging technologies, for example, for gender, age, racial and disability bias in algorithms should also be given due consideration. Risk assessment and mitigation measures should prevent unintended bias in the development and use of the weapon system, including gender, ethnicity, age and disability. Requirements to mitigate risks associated with bias in the algorithms used in lethal autonomous weapon systems could include comprehensive testing and reviews to identify and correct potential biases, training of operators, rigorous documentation of data set used in laws in conformity with national legislation, testing of algorithmic models against benchmarks that evaluate their operation against potential bias, as appropriate, transparency on how data sets are required and handled in conformity with national legislation, and last, particular care and specific measures with regard to the integrity, veracity and quality of data in conformity with national legislation. Thank you, Mr. Chair.","[...] With regard to the bias in the algorithm used in lethal autonomous weapon systems, the EU recognizes the critical role that data plays for AI based technologies. Social biases that could have a potential impact on emerging technologies, for example, for gender, age, racial and disability bias in algorithms should also be given due consideration. Risk assessment and mitigation measures should prevent unintended bias in the development and use of the weapon system, including gender, ethnicity, age and disability. Requirements to mitigate risks associated with bias in the algorithms used in lethal autonomous weapon systems could include comprehensive testing and reviews to identify and correct potential biases, training of operators, rigorous documentation of data set used in laws in conformity with national legislation, testing of algorithmic models against benchmarks that evaluate their operation against potential bias, as appropriate, transparency on how data sets are required and handled in conformity with national legislation, and last, particular care and specific measures with regard to the integrity, veracity and quality of data in conformity with national legislation. [...]",x,x,,x,,,,,,x,x,,x,,,,,,
INDIA,11:48:27,2024-08-30,0:02:32,"Thank you, Chair. I will limit my intervention to the section on risk mitigation. We have the following points to make. We would like the chapeau to the section to read as other measures to be implemented in accordance with national policies or framework to mitigate risks and further enhance compliance with IHL. Secondly, in the first para, we think that the phrase reliable expectation of how the weapon system will perform in the anticipated circumstances of its use is fairly vague and subjective and could be dropped unless there is a common understanding on what reliable expectations is achieved within the group. However, we would agree with the proposal made by the Russian Federation as also supported by the ICRC and many other Delegations on the need to include requirements for testing and evaluation of the systems to take place in real or realistic operational environments. On para 2, we would like to insert states should consistent with their legal obligations conduct legal reviews of laws to understand the weapons capabilities and limitations. On paragraph 3, we believe the word detecting here is not enough. We should add detecting and mitigating possible unwanted bias in datasets including to tackle data drift and here we would also agree with the point made by some other Delegations on unwanted bias being redundant and bias is presumably always unwanted. In the last para on para 4, we could bring uniformity with paragraph 3 by referring to detecting and mitigating unwanted automation. We would also like to propose a new measure based on guiding principle E agreed by the group which would read as when developing or acquiring new weapon systems based on emerging technologies in the areas of lethal autonomous weapon systems, physical security, appropriate nonphysical safeguards including cybersecurity against hacking or data spoofing, the risk of acquisition by terrorist groups and the risk of proliferation should be considered. We would be happy to send this in writing to the Chair. Thank you.","[...] On paragraph 3, we believe the word detecting here is not enough. We should add detecting and mitigating possible unwanted bias in datasets including to tackle data drift and here we would also agree with the point made by some other Delegations on unwanted bias being redundant and bias is presumably always unwanted. [...]",,,,x,,,,,,x,,,,,,,,,
HOLY SEE,11:58:56,2024-03-04,0:03:21,"In his message for the 2024 World Day of Peace, Pope Francis affirmed that the research on emerging technologies in the areas of so-called lethal autonomous weapons systems including the weaponization of artificial intelligence is a cause for grave ethical concern. As conflicts and divisions continue to affect humanity, there can be no escaping serious ethical questions related to the armament sector. The urgency of this matter is increasing due to the widespread development and use of armed drones including kamikaze and swarming drones. This has led to a lessened perception of the devastation caused and the burden of responsibility for their use resulting in an even more cold and detached approach to the immense tragedy of war. Autonomous weapon systems cannot be held morally responsible subjects. Human beings possess a unique capacity for moral judgment and ethical decision making that cannot be replicated by a complex set of algorithms and their capacity cannot be reduced to programming a machine that however intelligent remains a machine. If operating without any direct human supervision, such systems may make errors in identifying the intended targets due to unidentified biases induced by their self-learning capabilities. Therefore, it is crucial to ensure sufficient, meaningful and consistent human oversight over weapon systems equipped with artificial intelligence functions. Autonomous weapon systems lack the ability to comprehend the consequences of causing excessive or unnecessary suffering engaging in indiscriminate killing or adhering to the principles of humanity. These expressions which are codified in IHL require interpretation, good faith and prudent judgment, all of which are uniquely replaceable human traits. Mr. Chair, the Holy See has been a vocal supporter of negotiation of a legally binding instrument on laws and in the meantime of an immediate moratorium on their development or use. It is increasingly urgent to deliver concrete results given the pace of technological advancement. This is also mindful of the decade of dedicated discussions on laws. As we have been tasked to further consider and formulate the set of elements of an instrument to address the issue of laws, the Holy See encourages full use of this GGE to work specifically on the scope of prohibitions and regulations of weapon systems based on their degrees of autonomy, keeping firmly at the center of our deliberations the fundamental reference to the dignity of the human person. The development of ever more sophisticated weaponry even with the purpose of reducing collateral damage is not a long-lasting solution. War originates in the human hearts. Therefore, the real solution lies in the conversion to a culture of peace that places technological advancement at the service of humanity, of integral human development and of the common good. Thank you, Mr. Chair.","[...] Autonomous weapon systems cannot be held morally responsible subjects. Human beings possess a unique capacity for moral judgment and ethical decision making that cannot be replicated by a complex set of algorithms and their capacity cannot be reduced to programming a machine that however intelligent remains a machine. If operating without any direct human supervision, such systems may make errors in identifying the intended targets due to unidentified biases induced by their self - learning capabilities. [...] ",,,,x,,,,,,,x,x,,,,,,,
IRELAND,15:27:09,2024-03-05,0:03:09,"These fruitful and substantive discussions and the format you have chosen, we believe this is the way forward towards increasing convergence on the topic. Chair, autonomous weapons systems pose various challenges with regard to IHL. In particular, this includes factors such as unpredictability, the complex context based decisions necessary in the application of force, the need for qualitative human judgment and algorithmic biases. We need only point to the abundance of scientific, academic and civil society research contributions in these areas which underline time and again the value of human centrality. The pressing need to address the issue of explainability in AI and addressing bias and ethical considerations more broadly. A specific question of which elements, characteristics or characteristics would make autonomous weapons systems incompatible with IHL, we would point more broadly that this should refer to autonomous weapons systems that cannot distinguish between military and civilian targets. This is particularly relevant to autonomous weapons systems which rely on artificial intelligence techniques, on machine learning and which prevent the human user from being able to understand, predict or explain the system's output. This impossibility effectively results in a lack of control over the weapons effects, making it potentially indiscriminate by nature. We refer here to what is commonly called the black box challenge and which refers to the design of autonomous weapons systems that arrive at conclusions or decisions without providing any explanation as to how they were reached. Additionally, and as our colleagues in the ICRC have highlighted, in such systems, target selection is based on a generalized target profile that is unlikely to be able to account for the non-exhaustive range of contextual signals showing that a person is protected from attack. Whilst the user or commander may have made a general assessment that one or more people in the area constitutes a lawful target at the time of launching the autonomous weapons system, those people's actions, intentions and physical state and hence their qualification as a lawful target can change rapidly. The IHL rule on distinction requires assessment on the concrete circumstances prevailing at the relevant time and place. By definition, Chair, these types of systems cannot comply with IHL as their function cannot employ the IHL principles of distinction and proportionality. Chair, we believe that it is important that we have convergence in the room on this fundamental point. Systems that are fully autonomous by design or that do not include meaningful human control cannot comply with IHL. My Delegation remains flexible on how effective regulation is achieved, including through a combination of positive and negative obligations, legal and practical measures that ensure compliance with IHL. But the important point here is to ensure compliance. If there is doubt about a system, then it should not be used, developed or designed. I will leave it there now, Chair, but thank you.","[...] Chair, autonomous weapons systems pose various challenges with regard to IHL. In particular, this includes factors such as unpredictability, the complex context based decisions necessary in the application of force, the need for qualitative human judgment and algorithmic biases. We need only point to the abundance of scientific, academic and civil society research contributions in these areas which underline time and again the value of human centrality. The pressing need to address the issue of explainability in AI and addressing bias and ethical considerations more broadly. A specific question of which elements, characteristics or characteristics would make autonomous weapons systems incompatible with IHL, we would point more broadly that this should refer to autonomous weapons systems that cannot distinguish between military and civilian targets. [...]
",x,,,x,,,,,,,x,,,,,,,,
BELGIUM,16:43:32,2024-03-05,0:05:11,"Thank you, Mr. President. We would like to point out the core centrality of IHL and international criminal law as it relates to autonomous weapons. This centrality is the core of this discussion in my Delegation's opposition to matters related to these weapons systems. Belgium calls for a two-tier approach, first an explicit ban on certain types of autonomous weapons and the regulation of others such as an increasing number of Delegations within this group of experts have also said. Belgium's understanding of the two-tier approach acknowledges that all autonomous weapons systems are not equal as regards the legal and ethical potential implications by distinguishing systems that should be prohibited from those that should be regulated. The two-tier approach tries to create a more nuanced framework, a more efficient framework to meet the challenges of autonomous weapons systems. Belgium underscores the importance of significant human control on autonomous weapons systems and identifies the interaction between human and machine as essential in guaranteeing compliance with international human law, humanitarian law and international human rights law and international criminal law. Belgium recognizes that the interaction between humans and machines can take different forms and can take place at different stages of the life cycle of a weapon and so the regulatory and operational framework should be based on the process and it depends on the context, including the technical characteristics of the system, of the environment in which this is being used as well as the type of interaction between the human and the machine which should have significant human control. According to Belgium, the two-tier approach is based on a prohibition of autonomous weapons systems focusing on complete prohibition of a system that would be in IHL because it could cause suffering because it has indiscriminate effects or that they were created to cause or we might expect that they cause extended long lasting and serious damage to the natural environment as well as autonomous weapons systems that cannot be used in keeping with IHL. The latter type of weapon includes those who were created or used so as to ensure that their effects are not understood or explained well enough beforehand as well as those that do not have significant human control over their function such as the identification, the selection and the engagement of the target. The regulatory tier of the approach is for all autonomous weapons systems that are not completely prohibited and guarantees compliance with the humanitarian law including IHL and its fundamental principles. The distinction of proportionality and precaution means that we have to have guidelines and clear rules when it comes to requirements for human behavior during the development, the deployment or the use of these systems by focusing on maintaining significant human control but also in terms of responsibility and accountability throughout the entire life cycle. With that in mind, my Delegation supports the following parts. According to my Delegation, a weapon system that meets sub item 3, 4,5, 6, 7 and 8 is by definition a system that is incompatible with IHL regardless of its level of autonomy. Autonomous weapons systems that on the other hand are covered by sub items 1, 2 and the second part of the 7th item are more specifically related to autonomy and as such more relevant to the debate of the G but my Delegation supports these sub items as well. Lastly, my Delegation supports the item on the bullet point on the legal responsibility that the last one with which in and of itself is not incompatible with IHL but with the state's obligations under international law in broader terms. The state must guarantee accountability for the conception, the development, the deployment and the use of autonomous weapons systems in keeping with their obligations under international law and IHL. My Delegation also highlights the importance of taking into account the guiding principles B and D regarding the developers and manufacturers of these autonomous weapons systems given that the responsibility is on them for the conception and programming stages of an autonomous weapon system. This is particularly relevant for the problems related to the data biases that could affect decision-taking and the ill use of data or improper programming. Thank you.",[...] My Delegation also highlights the importance of taking into account the guiding principles B and D regarding the developers and manufacturers of these autonomous weapons systems given that the responsibility is on them for the conception and programming stages of an autonomous weapon system. This is particularly relevant for the problems related to the data biases that could affect decision - taking and the ill use of data or improper programming. [...] ,,,,x,,,,,,x,,x,x,,,,,,
MEXICO,15:44:40,2024-03-06,0:08:46,"Thank you very much, Chair. I am daring to speak first taking into account that I actually have to leave the room for a few moments later. So I would like to share with you my views on this particular issue. Now, this is an issue that the Mexican Delegation believes is very important because in order to effectively have effective regulations we have to really know what the risks are related incorporating autonomy into weapon systems. Now, Mexico looks at the issue of risks in a somewhat broader way than how you, Chair, reflected that in the list and I will try to move from a more general assessment to more specific comments. The first thing, of course, we would like to say once again that we have to look at the question of risks from a perspective of ethics. These are indeed machines that are taking decision on life and death. Autonomous weapon systems present serious ethical issues where there is a cognitive distance in space, time and understanding between the human decision of human beings and the use of arms and there is no moral barrier where there is a transfer from responsibility in decisions to machines. Second, and we are repeating something that we already said in our general comments, the risks have to be also looked at from a security point of view. Lethal autonomous weapon systems present the risks of asymmetric wars, disproportionate use of force, unintentional conflicts, escalation of conflicts, generates an arms race and also they can start interacting with other weapon systems including WMD, third point or third category of risks. These are related to the situation when laws are being developed, when they are not placed under meaningful human control so they are not in a position to comply with IHL or other existing legal standards due to their nature or the way they operate. In this aspect we believe that the following risks arise. The first, the risks of excluding human control, especially when it relates to technical requirements for the operation of lethal weapons systems through preprogrammed profiles, information on the environment through sensors analysis of the information being collected by sensors and applied through the existing or preexisting profiles. Mexico believes that such processes cannot replace human judgment required to implement the fundamental principles of IHL in specific contexts. Also, there are risks related to such contexts. IHL and their legal obligations under international law depend on decisions that are aware of the context and can adapt to changing circumstances. Autonomy represents risks where the specific context is not really known. Space and time are also not processed correctly when these weapons are used. Then also there are risks related to the predictability of the effects of laws related to the attempts of users to limit the way these machines are used in line with IHL for risks of intentionality. Laws can simply go beyond what humans are capable to process. There is a risk to presume their capacity to recognize the context and also to adapt in a predictable manner to new situations that were not planned during the design phase or during the evaluation phase. Number 5, risks relate to the human machine team. When we appreciate the need for human participation in the critical functions of machines, algorithms are extremely powerful to process extremely high levels of information, but this does not mean that they will carry out the correct causal analysis. Now, it is always necessary for humans to carry out necessary qualitative assessments and to determine legality or lawfulness. It has to be all adapted to changing circumstances and ensure control over the parameters, environment, and supervision during the use of such weapons. Now, where we have this link between humans and machines, still risks exist. For example, the situations where information presented is too complex to be analyzed by human beings before taking a decision, for example, identifying the deployment and use of models generated by artificial intelligence and it is also important to carry out that these limitations are even exacerbated when we are not just talking about one weapon system but when we have a network of such systems that operate in real time. So we are talking about a whole network of machines operating and this creates even more complicated situations. So this link implies that the participation of human beings will end up being simply nominal. Another challenge here is the situation related to operations where we see critical or unpredictable situations related to the application of force where humans are not in a position to have enough situational information to react in an adequate manner and to react promptly and maybe it will be even impossible for them to do anything. Then finally, the challenge of discrimination related to gender, race or other considerations and this can be related to the source of data in the system but also through the operation of processes related to algorithms. And very briefly on required measures, Mexico believes that we must distinguish between risks that require measures to simply avoid risks, risk avoidance measures which we believe are prohibitions and regulations or it is related to the discussions that we were having yesterday afternoon and this morning and also mitigation measures that may apply in a practical manner to analyze how this links up to norms of IHL and also how this will be linked to any norms that will be developed by this GG. Thank you.","[...] Then finally, the challenge of discrimination related to gender, race or other considerations and this can be related to the source of data in the system but also through the operation of processes related to algorithms. [...]",x,x,x,,,,,,,x,x,,,,,,,,
INTERNATIONAL COMMITTEE FOR ROBOT ARMS CONTROL,16:39:55,2024-03-06,0:02:43,"Thank you very much, Chair. My general comments here want to focus on the question of using agreed language when identifying risks. Indeed Delegations have emphasized the importance of using agreed language. In our understanding agreed language does not only mean language included in previous reports of this group but also includes language found in IHL and other binding sources of international law. There are three phrases or terminologies currently Chair on the screen in reply to your prompts that we want to focus on. The first phrase is the phrase unintended engagements which does not appear in the text of the CCW or the Geneva Conventions in the additional protocol. Does the risk of unintended engagements mean the risk of unintended killing of civilians or protected persons? If that is the case it is better to keep to IHL language on indiscriminate attacks or civilian harm. Second, phrases like algorithmic bias or simply bias do not sufficiently capture the risk being referred to. In its place it may be better, for example, to talk about the risk of discrimination or the risk of gender or racial discrimination, phrases which exist in binding legal norms. Third, the phrase risk mitigation, the phrase risk mitigation does also equally does not appear in the text of the CCW or the Geneva Conventions and the additional protocols. It is critical that we explain what we mean by this and how this aligns with established international legal obligations such as those that are found under international humanitarian law. Chair, some risks that are posed by laws already necessitate compliance with state obligations that exceed mere risk mitigation. Let me just give two examples, Chair. Let's talk about the risk of arbitrary deprivation of the right to life due to failure to comply with rules of distinction, for example. The obligation there is an obligation of prohibition on arbitrary deprivation of life. It is not an obligation to mitigate that particular risk. So when we say we are trying to mitigate risk, we need to be clear which particular risk we are referring to. If you look, for example, at the risk of racial and gender discrimination, the existing international obligations on state is to eliminate all forms of racial and gender discrimination. It is not to mitigate that particular risk. So it is very important in this particular regard for us to have some clarity. Lastly, it is evident, Chair, that certain risks posed by laws entail international obligations that surpass mere risk mitigation. While introducing new terms is permissible, caution must be exercised to avoid terminologies that undermine existing obligations. In cases where clear state obligations pertain to specific risks, any proposed risk mitigation should be regarded only as supplementary to existing obligations. With that, Chair, I thank you.","[...] My general comments here want to focus on the question of using agreed language when identifying risks. Indeed Delegations have emphasized the importance of using agreed language. In our understanding agreed language does not only mean language included in previous reports of this group but also includes language found in IHL and other binding sources of international law. There are three phrases or terminologies currently Chair on the screen in reply to your prompts that we want to focus on. The first phrase is the phrase unintended engagements which does not appear in the text of the CCW or the Geneva Conventions in the additional protocol. Does the risk of unintended engagements mean the risk of unintended killing of civilians or protected persons? If that is the case it is better to keep to IHL language on indiscriminate attacks or civilian harm. Second, phrases like algorithmic bias or simply bias do not sufficiently capture the risk being referred to. In its place it may be better, for example, to talk about the risk of discrimination or the risk of gender or racial discrimination, phrases which exist in binding legal norms. [...] ",x,,x,x,,,,,,,x,,,,,,,,
EUROPEAN UNION,16:46:31,2024-03-06,0:00:56,"Thank you, Chair. I would like to make a general comment and one of more specific nature on risk mitigation. The European Union is of the view that tailored risk mitigation measures including those across the life cycle should be adopted and implemented and with regard to the question of risk with regard to the development and use of laws and specifically on bullet 4, the EU recognizes the critical role that data plays for AI-based technologies. Social biases that have a potential impact on emerging technologies, for example, for gender bias in algorithms should also be given due consideration. Thank you.","[...] I would like to make a general comment and one of more specific nature on risk mitigation. The European Union is of the view that tailored risk mitigation measures including those across the life cycle should be adopted and implemented and with regard to the question of risk with regard to the development and use of laws and specifically on bullet 4, the EU recognizes the critical role that data plays for AI - based technologies. Social biases that have a potential impact on emerging technologies, for example, for gender bias in algorithms should also be given due consideration. [...]",,x,,x,,,,,,x,x,,x,,,,,,
UNITED STATES,17:30:03,2024-03-06,0:06:19,"Thank you, Mr. Chair. Just in the spirit of interactivity, I wanted to respond to the two points raised by our Distinguished Civil Society representative from ICRAC and just to provide a further clarification on our thinking on those very important topics. First with regard to the concept of unintended engagement, again, this is a core concept underlying US policy relating to autonomy and weapons systems. It is the thrust of our DoD directive 3000.09, autonomy and weapons systems is trying to reduce the probability of risks and failures that could lead to this outcome. The directive defines unintended engagement as and I will just read it out, the use of force against persons or objects that commanders or operators did not intend to be the targets of US military operations, including unacceptable levels of collateral damage beyond those consistent with the law of war rules of engagement and commanders' intent. And so the thrust of this policy is to really try and align the use of force in military operations as much as possible with commanders' intent and that includes commanders' intentions to reduce collateral damage or risk of harm to civilians as much as possible and to fully implement our IHL obligations. So we view this concept as a very important one in the context of autonomy and weapons systems and it is very useful for aligning the effects of the weapon with the intentions of the users. Therefore, just to respond to the question, we think it is relevant in particular with regard to lawful weapons, weapons that are not inherently discriminant or prohibited from use in all circumstances, but it is a consideration that we believe is applicable with regard to semi-autonomous and autonomous weapons systems that are lawful and may be used consistent with IHL rules. So we wanted to just provide that further clarification and information in response to that good question about unintended engagements. I think the other issue our distinguished colleague raised was with regard to our proposal with regard to unintended artificial intelligence pardon me, unintended bias in artificial intelligence capabilities and we in particular wanted to add the notion of unintended before bias. And I take the point that our colleague raised, which was a very good one, that there are obligations related to discrimination in international human rights law, that there certainly is a core IHL obligation requiring human treatment without adverse distinction based on a number of criteria and, of course, that is found in common Article 3 of the 1949 Geneva Conventions as well as other critical IHL instruments. And, of course, we believe these are very important obligations and need to be strictly adhered to. I think the legal point that we would make is that these obligations about refraining from discrimination or requiring human treatment without adverse distinction are applicable to people. And this is, of course, reflected in the 2019 GGE report where it was emphasized that rules of IHL apply to states, they apply to parties to a conflict, they apply to individuals, not machines. And so we do not see the reference to bias when we are talking about AI bias as referring to these legally prohibited categories of discrimination because we do not regard those legal obligations as applicable to the machines. When we are referring to AI bias, we are talking about a very technical phenomenon with regard to the programming of artificial intelligence capabilities. And I think it is a subject of much discussion and much expert interest. It is not necessarily connected to these legal categories that are protected from discrimination but the general phenomenon is that when machine learning or AI systems are used, they can reflect various biases that are present in the training data. And one of the key efforts to promote good practices with regard to responsible AI is to try and mitigate those biases. But, again, I think the key point I would emphasize is these are very technical biases, biases presented in the training data, and they are not necessarily connected to the things that we regard as protected criteria under international human rights law or international humanitarian law. So that is why we think the word "" unintended "" is important, because in the case of some AI models, there is a deliberate intent to bias the model's output and we think that can be quite positive a a positive effect including to enhance discrimination and precision in the use of force as required by IHL. So hopefully this is a helpful contribution to the discussion and I thank you, Mr. Chair and colleagues.","[...]I think the other issue our distinguished colleague raised was with regard to our proposal with regard to unintended artificial intelligence pardon me, unintended bias in artificial intelligence capabilities and we in particular wanted to add the notion of unintended before bias. And I take the point that our colleague raised, which was a very good one, that there are obligations related to discrimination in international human rights law, that there certainly is a core IHL obligation requiring human treatment without adverse distinction based on a number of criteria and, of course, that is found in common Article 3 of the 1949 Geneva Conventions as well as other critical IHL instruments. And, of course, we believe these are very important obligations and need to be strictly adhered to. I think the legal point that we would make is that these obligations about refraining from discrimination or requiring human treatment without adverse distinction are applicable to people. And this is, of course, reflected in the 2019 GGE report where it was emphasized that rules of IHL apply to states, they apply to parties to a conflict, they apply to individuals, not machines. And so we do not see the reference to bias when we are talking about AI bias as referring to these legally prohibited categories of discrimination because we do not regard those legal obligations as applicable to the machines. When we are referring to AI bias, we are talking about a very technical phenomenon with regard to the programming of artificial intelligence capabilities. And I think it is a subject of much discussion and much expert interest. It is not necessarily connected to these legal categories that are protected from discrimination but the general phenomenon is that when machine learning or AI systems are used, they can reflect various biases that are present in the training data. And one of the key efforts to promote good practices with regard to responsible AI is to try and mitigate those biases. But, again, I think the key point I would emphasize is these are very technical biases, biases presented in the training data, and they are not necessarily connected to the things that we regard as protected criteria under international human rights law or international humanitarian law. So that is why we think the word "" unintended "" is important, because in the case of some AI models, there is a deliberate intent to bias the model's output and we think that can be quite positive a a positive effect including to enhance discrimination and precision in the use of force as required by IHL. [...]",x,,x,x,,,,,,x,,x,,,,,,,
AUSTRALIA,10:35:53,2024-03-07,0:07:13,"Chair, we would now like to offer some comments on each of the sub-questions under topic 3. Turning to the first, being concrete risks with regard to the development and use of laws, we note that the GGE has been discussing risks of laws for some time now. As Delegations observed yesterday, there is a substantial amount of existing consensus language on risk in previous GGE reports that should be the starting point for our discussions on this topic. Several of these paragraphs from past consensus reports were drawn to our attention by Delegations yesterday. Over the course of past GGE sessions, different risks have been emphasized by different Delegations. My Delegation has largely been approaching risk through the lens of IHL compliance by focusing on mitigating the risk of unintended engagements and civilian harm. We think this is an appropriate we think this is appropriate in an IHL forum such as the CCW. As a general point, it is helpful to be clear on what risks relate to autonomous functions in weapons generally and which relate to specific AI technologies. Some risks listed here are uniquely related to the incorporation of artificial intelligence in weapons systems and we support the suggestion made yesterday by one Delegation to use more precise language where risk does relate to AI. For example, stating algorithmic bias in AI. We echo the observation yesterday that some of the points outlined are not risks of themselves. Rather, they are causes, consequences, or effects of risks. For example, the lack of human judgment in the design or use of a system may lead to risks of noncompliance with IHL and we think it is important to bear in mind this distinction when classifying the different risks. With respect to the first bullet point, we note that the risk of unintended engagements and loss of control of a system is a specific risk that is already been identified in previous GGE reports. Turning now to the second sub question regarding concrete risk mitigation measures with regard to the development and use of laws, we think this is an area where there is substantial common ground between GGE Delegations. If you look at the various proposals on the table, they all contain very similar risk mitigation measures. We heard yesterday suggestions on how to classify and categorize these risks and we think it would also be helpful to organize the risk mitigation measures into categories. We suggest one way of doing this could be to group them by the various measures taken at the various stages of a weapon's life cycle or the various stages of the life cycle of a weapon system. For example, risk mitigation measures to implement at the design, development and use stages. This would build on guiding principle G which notes that risk assessments and mitigation measures should be part of the design, development, testing and deployment cycle of emerging technologies in any weapon system. We also note that the GGE reports of 2018 and 2019 contain useful consensus language on risk mitigation measures. My Delegation endorses the views expressed by the ICRC yesterday on the need to distinguish legal obligations and measures to comply with legal obligations. We think this is a very important point. Several of the measures listed here are what we would categorize as IHL obligations and others are measures to ensure respect for IHL by a state's armed forces. So, for example, assessing expected collateral damage in relation to concrete military advantage would be a part of a proportionality assessment as required by IHL. We note that some of the risk mitigation measures outlined are quite vague. For example, the point on safety mechanisms, technical and organizational safeguards, et cetera, it is not clear to us what kind of safeguards or mechanisms are being referred to here. We also note that the term potential precaution could benefit from rephrasing or further elaboration as it may create confusion with the IHL obligation to take feasible precautions in attack. On the point about ensuring predictability, accountability, et cetera, we echo the point expressed by some Delegations yesterday that these are not risk mitigation measures but rather are goals or effects of other measures. We suggest risk mitigation measures should focus on the practical measures that can be implemented in order to reduce risk. Finally, we turn to the third prompt regarding concrete confidence building measures with regard to the development and use of laws. We understand confidence building measures as referring to measures that build mutual trust between states. CBMs are usually underpinned by the exchange of relevant information among states so as to clarify others' intentions, avoid suspicion and reduce misperceptions. As you, Chair, have set out in your prompt, we would like to see risk mitigation measures and CBMs as conceptually distinct. Given how we understand CBMs, we would like to see some of the actions listed sorry, we would see some of the actions listed as somewhat misplaced under this particular heading. For example, ensuring military documents and training for military personnel are updated and measures enabling after action review of a weapon system, those measures are in our view good practices for ensuring compliance with IHL by states armed forces rather than what we would typically categorize as a confidence building measure. We are of the view that the ICRC's point about the need to be clear about what measures are legal obligations and which go beyond this is equally relevant to the points listed under this prompt. We are pleased to see included in this list the voluntary exchange of relevant best practices on legal reviews and we note that this was also referenced in paragraph 23 of the 2023 laws GGE report. Thank you, Chair.","[...] As a general point, it is helpful to be clear on what risks relate to autonomous functions in weapons generally and which relate to specific AI technologies. Some risks listed here are uniquely related to the incorporation of artificial intelligence in weapons systems and we support the suggestion made yesterday by one Delegation to use more precise language where risk does relate to AI. For example, stating algorithmic bias in AI. [...]",,,,x,,,,,,,x,,,,,,,,
GERMANY,10:56:37,2024-03-07,0:01:06,"Germany is also of the view that the risks related to the development and use of laws may be seen to fall under certain categories. We are in favor of that concept. We see risks related to unpredictable system behavior, algorithmic biases, reliability which may inter alia lead to unintended biases such as on gender aspects. Adequate testing, legal reviews and adequate training for human decision makers and operators to understand the system's effect and its likely interaction with its environment are in our view key to countering these risks. Other risks may be related to a possible loss of human control, the lack of human judgment or lack of accountability. We also see the risk of development, deployment and use of laws by non-state actors. In our view, the second tier focused on regulations could address the aforementioned issues and the concept of their categorization. Thank you.","[...]Germany is also of the view that the risks related to the development and use of laws may be seen to fall under certain categories. We are in favor of that concept. We see risks related to unpredictable system behavior, algorithmic biases, reliability which may inter alia lead to unintended biases such as on gender aspects. Adequate testing, legal reviews and adequate training for human decision makers and operators to understand the system's effect and its likely interaction with its environment are in our view key to countering these risks. [...]",x,x,,x,,,,x,,,x,,x,,,,,,
BELGIUM,11:01:15,2024-03-07,0:03:42,"Thank you very much, Chair. Chair, the issue of the autonomous nature of weapons systems requires specific questions to be defined, especially looking at how this relates to IHL in terms of the use of these kinds of weapons. My Delegation should like to thank you for your questions and for having brought together the responses of the Delegations and should also like to thank the Delegations who have provided responses. Belgium should like to more specifically provide support at the following points with regard to the first question. On the first point, loss of human control, Belgium prefers the terminology loss of human control to the previous terminology given that the unintentional actions and the loss of control in the systems are included in the notion of loss of human control in general terms. As far as the second point is concerned, my Delegation supports the idea of unpredictable system behavior or unpredictability. We support that notion of unpredictability, whilst bearing in mind the discussions that we have had this week and whilst understanding that this terminology may be considered as lacking precision, I am talking about unpredictability here, we nevertheless support the first which we think covers the same concept whilst being a little bit more specific. We prefer the use of unpredictability rather than behavior to avoid the impression that a machine or system can have behavior attributed to it and, therefore, we should be avoiding anthropomorphizing weapon systems. In terms of biases, which was extensively discussed already, the use of autonomous weapon systems by the very nature of those systems involves the risks of bias and these are various types and not only attributable to artificial intelligence but especially to the developers themselves. My Delegation, therefore, prefers data biases in general given that they may be biases even when using a large amount of data so my Delegation supports the intervention made by the Delegation of Austria yesterday with regard to biases. For us, by definition, a bias is unintentional and includes, therefore, the mention of unintended bias rather than that terminological specificity. My Delegation supports the notion of deployment and use and acquisition by unauthorized users including terrorist groups. My Delegation welcomes the inclusion of a point relating to a danger to non-state actors. In particular, we would be sensitive to the last suggestion which we think integrates larger spectrum of users without going into discussions into what is and what is not a non-state actor. But it is not simply acquisition but also deployment and use of the systems by nonauthorized users which represents a risk. So deployment and use and acquisition by unauthorized users including terrorist groups is what we would support. Thank you very much, Chair.","[...] In terms of biases, which was extensively discussed already, the use of autonomous weapon systems by the very nature of those systems involves the risks of bias and these are various types and not only attributable to artificial intelligence but especially to the developers themselves. My Delegation, therefore, prefers data biases in general given that they may be biases even when using a large amount of data so my Delegation supports the intervention made by the Delegation of Austria yesterday with regard to biases. For us, by definition, a bias is unintentional and includes, therefore, the mention of unintended bias rather than that terminological specificity. [...] ",,,,x,,,,,,x,,,x,,,,,,
CANADA,11:30:55,2024-03-07,0:02:04,"Thank you, Mr. Chair. My Delegation has asked for the floor to thank Ireland for presenting the working paper on addressing bias in autonomous weapons including in particular as algorithmic bias relates to gender. There was no question as to the importance for Canada to support the paper. As we heard yesterday from the panelists on the gender side event which Canada was also pleased to cosponsor, the question of gender in AI and indeed in military automated applications is not a fringe discussion to our deliberations here but indeed it is important to the discussions on laws specifically in the context of biases. Mr. Chair, Canada recognizes that maintaining international peace and security requires an inclusive approach that considers the perspectives of all persons and that gender balance and gender considerations have a positive impact on decision making processes related to nonproliferation, armed controls and disarmament issues at large. As we all know, it is not enough to simply have more women in the room. We must continue to recognize that biases including gender biases at the development stage of a weapon system can affect different populations and identities in different ways including women and gender diverse individuals. We would also like to acknowledge the wealth of research and data produced by international organizations in the field of gender biases in automation in weapon systems. We welcome the work and research from these organizations such as the reports produced by UNIDIR and believe international organizations have a pivotal role in our discussions within the GGE on this topic. Canada is pleased to be providing support to UNIDIR's gender and disarmament programs in the area of conventional weapons and ammunition. This support provides an opportunity for UNIDIR to explore greater connections between disarmament and gender equality across all disarmament fields. Thank you.","[...] My Delegation has asked for the floor to thank Ireland for presenting the working paper on addressing bias in autonomous weapons including in particular as algorithmic bias relates to gender. There was no question as to the importance for Canada to support the paper. As we heard yesterday from the panelists on the gender side event which Canada was also pleased to cosponsor, the question of gender in AI and indeed in military automated applications is not a fringe discussion to our deliberations here but indeed it is important to the discussions on laws specifically in the context of biases. Mr. Chair, Canada recognizes that maintaining international peace and security requires an inclusive approach that considers the perspectives of all persons and that gender balance and gender considerations have a positive impact on decision making processes related to nonproliferation, armed controls and disarmament issues at large. As we all know, it is not enough to simply have more women in the room. We must continue to recognize that biases including gender biases at the development stage of a weapon system can affect different populations and identities in different ways including women and gender diverse individuals. We would also like to acknowledge the wealth of research and data produced by international organizations in the field of gender biases in automation in weapon systems. We welcome the work and research from these organizations such as the reports produced by UNIDIR and believe international organizations have a pivotal role in our discussions within the GGE on this topic. [...]",,x,,x,x,,,,,x,x,,x,,,,,,
BELGIUM,11:37:54,2024-03-07,0:05:17,"Thank you, Chairman. Chair on risk mitigation measures, my Delegation should like to recall guiding principle E of the GGE in favor of the universalization of the legal review of all weapon systems including autonomous weapon systems as early as possible in terms of the development or acquisition of weapon systems in line with Article 36 of AP1 in the Geneva Convention as the Swedish Delegation just recalled the examination of lawfulness should in our opinion be renewed if there are any significant changes made to the weapon in terms of its essential functions or indeed its effects. That is particularly important with regard to autonomous weapon systems because these systems may include machine learning or automatic intelligence capacities. For my Delegation guaranteeing the compliance with international law including IHL and its fundamental principles of distinction, proportionality and precaution means evolving clear standards and guidelines in terms of design and human behavior with regard to the deployment and use of these systems whilst highlighting maintaining meaningful human control but also in terms of responsibility and accountability throughout the life cycle. These requirements may include but are not limited to limits on the type of target that the system can engage, limits on the duration, the geographical scope of the use of the weapon system, clear procedures to guarantee that the commander and the human operators are properly trained and informed about the system and that their behavior and their effects and requirements with regard to the interaction between humans and the machine and the intervention and deactivation at an appropriate time if necessary as well as test procedures, certification and rigorous validation procedures, requirements with regard to safe registration systems as well as limits on the capacity of the weapon system to in live times implement aspects in its critical functions. Whilst taking into account the position my Delegation should like to provide its support to the following points that you highlighted in your document. Rigorous testing and evaluation. We think it is not only about undertaking tests but also being able to undertake evaluation and assessments with regard to the results of these tests. We think the first version of this point is more specific and precise. My Delegation also supports the mentions of script qualifications assessment validation procedures and, of course, legal reviews whilst maintaining the fact that these issues relating to lawfulness look at all weapon systems including autonomous weapon systems and their particularity should be once again applied to each amendment or modification made to the weapon system. My Delegation also supports the question on adequate training of personnel. Education in international humanitarian law is also recommended by my Delegation for any deployment and not only that including autonomous weapons but also semi -autonomous weapons. Appropriate engagement in another point from my Delegation appropriate rules of engagement encapsulates the following proposal on the same point. In terms of biases, the question has already been examined during the risk-related issues but my Delegation would support that the proposals that have been used here to reduce bias and unattended bias is probably that which would enjoy our support most in terms of data biases. This might be more beneficial if it is generic and would avoid specifying those biases or unintentional biases that we already mentioned previously. Finally, Chairman, although for Belgium the adoption of an international legally binding instrument looking to prohibit and regulate autonomous weapons systems is necessary, my Delegation nevertheless thinks that national legal reviews are the best way to fully implement the obligations contained in international instrument whether it be binding or not at a national level. In terms of the following up the various elements contained within an instrument that may be adopted by the high contracting parties of the CCW, it would be possible to establish a mechanism to exchange good practices and the challenges thrown up during the review. Thank you.","[...] In terms of biases, the question has already been examined during the risk - related issues but my Delegation would support that the proposals that have been used here to reduce bias and unattended bias is probably that which would enjoy our support most in terms of data biases. This might be more beneficial if it is generic and would avoid specifying those biases or unintentional biases that we already mentioned previously. [...]",,,,x,,,,,,x,,,,,,,,,
IRELAND,11:08:05,2024-08-26,0:03:18,"Good to be joining the group for the first time. Thank you very much, Ambassador Indien-Bosch for efforts throughout the intercessional period including through the preparatory informal consultations. We emphasize strongly the importance of inclusive consultations throughout incorporating the expertise of observers including the ICRC and civil society organizations. Ireland underlines the urgency of the GGE's work. We note the UN Secretary General's recently published report underlying the strong sense that time is running out for the international community to take preventive action on this issue. We recall its call for CCWI contracting parties to fulfill the GGE's mandate as soon as possible which is ideally by end 2025. In addition to the UNSG's report, the various complimentary initiatives developing outside the GGE also provide impetus for our work here and reaffirm how we need to address autonomous weapons systems across a range of issues including international human rights law, international criminal law, ethics, human dignity and security. Chair, we welcome your rolling text which offers a concrete basis for our deliberations and allows for a focused discussion on outcomes. We urge all Delegations to engage on that basis, noting again the urgency of our work rather than repeating theoretical debates which we have held over the years. Chair, Ireland reiterates that human beings must make and remain accountable for decisions over the use of lethal force and must exert full control over lethal weapons systems. Affording machines the ability to make decisions over life and death is inconsistent with international law, in particular IHL, since humans must determine the lawfulness of an attack and raises serious ethical concerns. We join the global calls including by the UN Secretary General and the ICRC President to urgently establish legally binding rules and guidelines on autonomous weapons systems. We draw attention to the particular risk of bias in autonomous weapons systems, in particular that algorithms and related machine learning risk repeating, amplifying or contributing to unjust biases including regarding race, gender, age and disability. This unique challenge must be addressed when discussing prohibitions and regulations. We encourage high contracting parties to consult the joint working paper on bias submitted in the March session which is still open for endorsement and invite HCPs to a side event on the matter on Wednesday. Thank you very much.","[...] We draw attention to the particular risk of bias in autonomous weapons systems, in particular that algorithms and related machine learning risk repeating, amplifying or contributing to unjust biases including regarding race, gender, age and disability. This unique challenge must be addressed when discussing prohibitions and regulations. We encourage high contracting parties to consult the joint working paper on bias submitted in the March session which is still open for endorsement and invite HCPs to a side event on the matter on Wednesday. [...]",x,x,,x,,,,,,,x,,,,,,,,
PERU,11:59:17,2024-08-29,0:03:10,"Thank you, Chairperson. My Delegation agrees with those that have expressed the view that this part on prohibitions and restrictions is the most significant of the work before us this week. I would like to make some specific comments regarding paragraph 5. My Delegation agrees with Brazil, Ireland and the ICRC that it is preferable to use human control rather than appropriate control. On paragraph 6, my Delegation also supports the same Delegations, Brazil, Ireland and the ICRC and possibly others in the proposal that this be worded as a prohibition as my colleague from El Salvador mentioned just now. Peru is also one of the countries that presented the draft protocol 6 which includes prohibitions and restrictions along these lines. Finally, I would like to support the idea put forward for an additional paragraph that was mentioned just now by the ICRC and that would be on the need to prohibit autonomous weapons designed in such a way that they can be activated in the human presence. As was stated, this is one of the conclusions of the document prepared by the Presidency of the Vienna Conference. The selection of persons is a very pressing issue, targeting of human beings. Autonomous weapons that distinguish certain groups of persons from others would give rise to issues of partiality in the data and algorithms on which they are based. Thus, we consider that the idea of the proposal made by the ICRC for the additional paragraph is relevant. Thank you.","[...] The selection of persons is a very pressing issue, targeting of human beings. Autonomous weapons that distinguish certain groups of persons from others would give rise to issues of partiality in the data and algorithms on which they are based. [...]",,,,,,,,,,x,x,,,,,,,,
AUSTRIA,11:11:07,2024-08-30,0:04:36,"Thank you very much and we also have some comments on this section. I think as a general comment, I think this section on the one hand helps us to put out some additional measures that states should take to some extent with existing legal obligations such as the ones to undertake legal reviews but then it also could help us to spell out some more technically related measures that we also see as necessary to comply with IHL. On the first para, we also agree with the Russian Federation. I think this is a good addition. On the second para and the issue of legal reviews, we are also very much in line with what the SCRC has just suggested. What we see as a problem here with this para is that it is going below the current obligations of legal reviews and how they are conducted. I mean, to understand the weapons capabilities is not necessarily the main point of conducting such a legal review. For most of us who are part of additional protocol 1, there is a very specific formulation what we are supposed to do under those legal reviews and this formulation additional protocol 1 is very clear and very relevant when it comes to autonomous weapon systems. And we have, as the SCRC has pointed out, we have agreed language in that respect which would be the guiding principle E and also last year's formulation that we have used in a shortened version which is also based on previous formulations I think that we have already there. There is a lot of text. I think we should use this agreed language on legal reviews and the proposal that we separate this current para that also has very valuable elements about understand the weapons capabilities and so on could be separated out as a measure on its own because it is also not clearly linked with the legal reviews as they are set out under additional protocol 1 or national measures that states are already undertaking. We could change this to states should take measures or states should conduct reviews to understand weapons capabilities and limitations. I mean, those are two options. Otherwise, as I said at the beginning, this para would be too limiting. Then we have some other proposals. One is, I mean, we would have proposed something in a separate para. I think the SCRC wanted to link it with the legal reviews but definitely there should be one after the other. The para that we wanted to propose reads states should constantly review and reassess possible changes and modifications in the systems functioning, in particular in relation to machine learning and the datasets the systems functions are based on. We will send this in writing. Then at the very end or in the next para we would like to delete unwanted bias because we do not think that there is wanted bias. In our interpretation bias is always a flaw in the system that you should prevent to be included. We know that other states are seeing this differently but unwanted bias would lead us to the conclusion that there is a kind of bias that we want. I know that there is a discussion still to have upon this, but our interpretation is more technical, less sociological. We always consider bias as something that is a flaw in the system and does not lead to results which were originally intended when you have designed the systems which would in our context make it a violation of IHL. Then we would at the end like to add some new paras that are also important and are going in to a direction when it comes to the technological part of autonomous weapon systems. The first one would read states should ensure the integrity, quality and diversity of datasets they use. And the next one should be states should ensure the safety of such weapon systems in particular with regard to cybersecurity. That is all from our side on this part. Thank you.","[...] Then at the very end or in the next para we would like to delete unwanted bias because we do not think that there is wanted bias. In our interpretation bias is always a flaw in the system that you should prevent to be included. We know that other states are seeing this differently but unwanted bias would lead us to the conclusion that there is a kind of bias that we want. I know that there is a discussion still to have upon this, but our interpretation is more technical, less sociological. We always consider bias as something that is a flaw in the system and does not lead to results which were originally intended when you have designed the systems which would in our context make it a violation of IHL. [...]",,,,x,,,,,,,,,,,,,,,
ENCODE JUSTICE,12:17:41,2024-08-30,0:01:02,I will try to speak a lot slower than I did in my previous intervention. We were happy to hear our considerations on unintended versus intended bias and continuous review of systems echoed by so many Delegations this morning. And we also want to express support for the proposal of the UK just now to take out the word possible additional to the word unwanted in paragraph 3. And we also welcome the echoing of the Delegation of India of drift that can appear over the life cycle of an algorithm we brought forward yesterday. Our remark would be rather short and would only suggest to refer to model drift instead of data drift in the text as proposed by the Delegation of India. As data drift is only one example of model drift that can appear over the life cycle of that algorithm that would lead to the decay that is brought forward by our colleague of the University of Cambridge as well. Thank you.,"[...] We were happy to hear our considerations on unintended versus intended bias and continuous review of systems echoed by so many Delegations this morning. And we also want to express support for the proposal of the UK just now to take out the word possible additional to the word unwanted in paragraph 3.
And we also welcome the echoing of the Delegation of India of drift that can appear over the life cycle of an algorithm we brought forward yesterday. Our remark would be rather short and would only suggest to refer to model drift instead of data drift in the text as proposed by the Delegation of India. As data drift is only one example of model drift that can appear over the life cycle of that algorithm that would lead to the decay that is brought forward by our colleague of the University of Cambridge as well. [...]",,,,,,,,,,x,x,,,,,,,,
RUSSIAN FEDERATION,12:19:04,2024-08-30,0:05:38,"Thank you, Chairman. We would like to make a set of comments in order to react to what was said by other Delegations. We agree with the proposals from a set of Delegations that are more detailed and use more broadly the guiding principles. In particular, the principle E, F and G from the list which was agreed upon in 2019. In our view, the guiding principles or guidelines explain the issue of legal reviews and they also relate to the problem of risk reduction which a number of Delegations have commented on. As regards the issue of bias, we should not make this problem complicated. We believe that the key idea in this context is linked to the need to reduce the number of algorithmic bias during the functioning of a given system to try to divide up into several categories and types of this bias in this context does not really make sense and it will only further complicate the problem. As we know, furthermore, with respect to this problem, all aspects have not enjoyed consensus far from that for many years. We have been considering this issue and there are an entire set of aspects mentioned by Delegations as regards where there are disagreements and no consensual decision has been adopted. Our Delegation doubts that we can find a solution on this issue, these issues. Therefore, we believe that there is no need to go into more detailed discussion of this. We need to focus on resolving the issue that we are all talking about here, the need to reduce any type of malfunctioning in the operation of these systems. The wording that would cover the need by states to take measures to discover and reduce such malfunctions we believe is flexible and useful in the search for a consensus solution. In conclusion, we would like to respond to proposals by a number of Delegations on developing some standardized decisions at the national level or measures at the national level on the operation of such systems in order to ensure compliance with IHL. The Russian Federation continues to hold the principal position to the effect that the prerogative of selecting specific measures to ensure human control which is directly connected with ensuring fulfillment of the obligations and principles of international humanitarian law remain with states and states themselves must determine what measures at the national level, what standards at the national level should be taken by them in order to guarantee compliance with international humanitarian law. Furthermore, the states' parties bear full responsibility for their actions in that connection including for the measures which they may apply at the national level. Trying to dictate to states what measures they should be taking at the national level we believe is unwise and it is unlikely to be something that could be discussed in this group. Thank you.","[...] As regards the issue of bias, we should not make this problem complicated. We believe that the key idea in this context is linked to the need to reduce the number of algorithmic bias during the functioning of a given system to try to divide up into several categories and types of this bias in this context does not really make sense and it will only further complicate the problem. As we know, furthermore, with respect to this problem, all aspects have not enjoyed consensus far from that for many years. We have been considering this issue and there are an entire set of aspects mentioned by Delegations as regards where there are disagreements and no consensual decision has been adopted. Our Delegation doubts that we can find a solution on this issue, these issues. Therefore, we believe that there is no need to go into more detailed discussion of this. We need to focus on resolving the issue that we are all talking about here, the need to reduce any type of malfunctioning in the operation of these systems. The wording that would cover the need by states to take measures to discover and reduce such malfunctions we believe is flexible and useful in the search for a consensus solution. In conclusion, we would like to respond to proposals by a number of Delegations on developing some standardized decisions at the national level or measures at the national level on the operation of such systems in order to ensure compliance with IHL. Trying to dictate to states what measures they should be taking at the national level we believe is unwise and it is unlikely to be something that could be discussed in this group. [...]",,,,x,,,,,,,x,,x,,,,,,
UNITED STATES,17:11:46,2024-03-06,0:07:32,"Thank you, Mr. Chair. We are going to try to offer reactions on both the risk question and the risk mitigation measures question in this intervention and then we will return to the confidence building measures at a later time. Now on the risks themselves, we agree with many of the overall points that others including the UK and Switzerland have made today. The first is that many of these risks are subsidiary risks of broader and overarching risks on the list and the second is that we already have a significant body of consensus language on risks, including in the guiding principles and in prior GGE reports. For our Delegation, the most important risk to focus on and seek to mitigate is the risk of unintended engagements which is closely related to the final bullet on risk of harm to civilians. We consider many of the risks identified in the bullets as related to these two overarching risks and do not think the risk of unintended engagements should be merged with the loss of control over a system or the loss of human control as these are not the same concepts. To respond to the point that the Delegate from ICRAC just raised, the US Delegation considers the risk of unintended engagements to mean that the weapon system is engaging targets that the system operator did not intend to engage. So in that sense it is broader than just civilian harm. The loss of control over a system does not necessarily lead to unintended engagements and unintended engagements may be caused by a variety of factors including but not only through the loss of control over a system. Now, just briefly with respect to the point about using previously agreed language, we did want to reiterate the point made by others that we have consensus language on many of these risks. So just for example, with regard to the risk noted in the list as loss of human control in the first bullet, we would point to the 2019 report where we have several paragraphs that reflect this general concern such as guiding principle D which refers to a responsible chain of human command and control. Another option consistent with the paragraph that we agreed in our 2023 report in paragraph 21 C could simply be to omit the word human and refer to a loss of control. With regard to the second bullet that begins with unpredictable system behavior, we think this bullet needs a little bit of clarification. We should focus on unpredictability that may be related to unintended engagements or risk to civilians. So again, this is a subsidiary to those two broader risks. Finally, with regard to the third bullet on reliability, this is not a risk as we see it. So we would remove that from the list. And then lastly on the issue of bias, we would recommend describing the risk as unintended bias in AI capabilities. We suggest to add the word unintended to be more specific about the type of bias that are problematic. Indeed, distinction is an important IHL requirement. We also think it is appropriate to refer to AI capabilities rather than just generally to AI. Now moving on to risk mitigation measures, this is a topic where the GGE has had many fruitful discussions in the past. In looking at the various proposals that Delegations have submitted since 2022, there is really a great deal of overlap and similarity in identifying concrete actions that states should take to ensure and promote compliance with IHL. And we also again like the prior topic already have a body of consensus language that we can expand on. Many of the concrete measures on this list from rigorous testing and verification to weapons reviews to understandable human machine interfaces are included in the prior two prior joint proposals cosponsored by the United States and also in consensus GGE language including the 2019 and 2023 GGE reports. While not specifically framed as a risk mitigation measure, paragraph 22 of last year's GGE report states that when necessary, states should in derailia limit the types of targets that the system can engage and limit the duration, geographical scope and scale of the operation of the weapons system. Now, in addition to these general points, we did just want to make a few specific comments on a handful of the bullets. We will start by addressing bullets 9 and 10 together. The ninth bullet addresses the use of a weapon within a responsible chain of human command and control. Our draft article's proposal includes a number of measures that states should implement to ensure comprehensive accountability for the use of autonomous weapons systems including that states should only deploy laws within the state's general framework for implementation of IHL such as operation of such system within a responsible chain of human command and control. Thus, as we see it, the tenth bullet which says permit attribution of responsibility to individuals and states is not a separate risk mitigation measure but rather a goal or effect of the measure addressed previously. With respect to the 11th bullet which begins with the phrase alternative tools to ensure human control, again, we do not see these as measures but rather as a potential goal or effect of other measures. As our Delegation has said many times, human control and human supervision and intervention are not legal requirements. The very purpose of autonomy is to allow machines to carry out tasks that humans have previously performed. That said, measures such as controls and limits on targets or the incorporation of self-destruct or self-neutralization mechanisms can be risk mitigation measures to reduce the risk of unintended engagements. These measures can be implemented through mechanisms or software programming in the weapon system but they could also be included as fundamental attributes in the design of the weapon system such as the maximum amount of fuel available to the system which would affect its operating duration or its maximum geographic scope or by the number of munitions that a weapon system carries. Lastly, with respect to the 12th bullet which begins with ensure predictability, traceability and accountability and also mentions things like reliability and explainability, again, these are not measures but rather a goal or effect of other measures that can be adopted to effectively implement IHL. Our Delegation believes that we must give precise and implementable guidance to states on what measures they can take to mitigate the risk of unintended engagement in the use of laws. Ensuring predictability is not a measure but many of the other measures on the list from rigorous test and evaluation to validation and verification measures, these are measures that we think would help address the underlying concerns that Delegations who suggested this language have in mind. Thank you, Chair.","[...] And then lastly on the issue of bias, we would recommend describing the risk as unintended bias in AI capabilities. We suggest to add the word unintended to be more specific about the type of bias that are problematic. Indeed, distinction is an important IHL requirement. We also think it is appropriate to refer to AI capabilities rather than just generally to AI. [...] ",,,,x,,,,,,,,,,,,,,,
REPUBLIC OF KOREA,17:27:27,2024-03-06,0:01:49,"Thank you, Chair. I did not want to prolong the discussions, but I wanted to make the following comments, knowing that this might be the last chance to speak today. A number of risks and risk reduction measures listed seem to be directly related to elements that would form an integral part of regulations that would be required to ensure the lawfulness of development and use of laws. As such, and as Austria and the United States propose, it would be useful to do some boxing work work and also differentiate between the essentials and subsidiary considerations and countermeasures. To this end, after we complete the initial round of discussion to add to and delete from the current list on Topic 3, we could work on coming to a common or shared understanding, if not agreement, on the main categories of prohibitions and regulations first and then deal with how we may address other subsidiary yet important considerations such as ensuring safeguards against the data theft, alteration and proliferation at a later stage. And on a final note and before I close, we would support the addition of the word unbiased in front of bias, sorry, in front of unintended, unintended in front of the word bias because some intended bias is necessary to ensure the lawfulness of laws in order to obtain the intended effect. In other words, to ensure that the laws that we employ target the targets that we have preselected, some bias would be necessary in some cases. Thank you","[...]And on a final note and before I close, we would support the addition of the word unbiased in front of bias, sorry, in front of unintended, unintended in front of the word bias because some intended bias is necessary to ensure the lawfulness of laws in order to obtain the intended effect. In other words, to ensure that the laws that we employ target the targets that we have preselected, some bias would be necessary in some cases. [...]",,,,x,,,,,,,,,,,,,,,
INTERNATIONAL COMMITTEE FOR ROBOT ARMS CONTROL,11:54:37,2024-03-07,0:02:43,"Thank you very much, Chair. Just one perhaps comment on confidence building measures. Perhaps the preliminary question that we may want to pause is whose confidence is being built when we are referring to these confidence building measures? I ask this question particularly because the impact of laws in terms of the risks that we are identifying do not necessarily impact everyone in the same way. What we mean by this, the risks or how they relate to those who are building laws, those who will be using laws and those who will be on the receiving end of these particular laws are different. So the question then of whose confidence is being built becomes very important. In this particular regard, it is important at least in our view that when we are talking of confidence building measures, the measures that we discuss are inclusive of all individuals or all stakeholders will be affected. So the question of inclusivity in this particular regard is very important as part of confidence building measures. Now, when we are talking of inclusivity, in our view, this should not only begin after the weapons have been deployed or are now being used, but also including now as we are discussing policies on laws. As a question, Chair, just a look on the, for example, the names of stakeholders or states on the screen when we are discussing, you can see the dynamic in terms of who has been participating actually in this process and what this will mean in the future in terms of building confidence in in regulation of these systems. Lastly, Chair, I wanted to also raise a note on the question of specifying risks than being general when we are talking of risks. The importance of this is that there are certain risks which we need really to be concrete about. We have heard some stakeholders, for example, suggesting that we need to be general when it comes to risks such as bias. We believe that that approach perhaps is not that effective because inclusivity, there are certain issues that concern certain stakeholders that we need actually to be more concrete. Thank you very much, Chair.","[...] Lastly, Chair, I wanted to also raise a note on the question of specifying risks than being general when we are talking of risks. The importance of this is that there are certain risks which we need really to be concrete about. We have heard some stakeholders, for example, suggesting that we need to be general when it comes to risks such as bias. We believe that that approach perhaps is not that effective because inclusivity, there are certain issues that concern certain stakeholders that we need actually to be more concrete. [...]",,,,x,,,,,,,,,,,,,,,
GERMANY,11:33:01,2024-03-08,0:02:51,"Thank you, Mr. Chair. Thank you for giving us the possibility to come back on important issues we discussed at the beginning of the week as well as for your well thought through remarks that were circulated last night. We find the way in which you have structured our discussions in the course of this week highly useful and we very much appreciate the efforts of you and your team in this regard. As for your suggestions on revisiting topic 1, characterizations and definitions, we would have seen some merit in the use of the wording technologies that enable laws in order to avoid a discussion of certain systems that may at one point in time be outpaced by technology. But we note that numerous Delegations prefer to use a more narrow wording as you stated in your comments and we are ready to be flexible in this regard. Our Delegation would prefer to keep the notion once activated or after activation as the colleagues from the US proposed for the following reasons. In our view any system that is not fully autonomous but features autonomy in certain functions will be put into a state of operation by an action. We therefore see the activation part as one of the relevant aspects of an emerging technology in the area of laws and crucial in its own right as it represents the point in time after which a system will fulfill the task it was specifically designed for. We align ourselves with the remarks just made by France in this regard and we also support the wording without further human intervention. We would see some merit in sticking to agreed language and keep identification keep the identification part as one of the essential characteristics of laws, identify, select and engage. In our view the identification forms an important part of the critical functions that may be undertaken by a weapon system featuring autonomy in various functions including because it may be relevant with regard to the challenges related to underlying unintended biases. On the question whether to deploy or apply force we consider apply force or apply lethal force as the colleagues from the US proposed to be better descriptive of the effect that the weapon systems under discussion could achieve through their actions as was outlined by the distinguished colleague from Australia. Our Delegation agrees with your suggestion to discuss some more fundamental questions during the intercessional period or at the following GGE session. Thank you.",[...] In our view the identification forms an important part of the critical functions that may be undertaken by a weapon system featuring autonomy in various functions including because it may be relevant with regard to the challenges related to underlying unintended biases. [...],,,,x,,,,,,,,,,,,,,,
LUXEMBOURG,12:11:03,2024-03-08,0:02:18,"But first, Luxembourg. Thank you, Chair. Also let me quickly echo other Delegations in thanking you and your team as well as the Secretariat for the way you are guiding our work this week. We understand the need and wish of Delegations to have a working definition or characterization to scope of our discussions here in the GGE, even though without the definition being final. We are also in favor of an inclusive approach to characterizing and defining laws which would be leaving the necessary space to include emerging technologies and evolving technologies and to formulate pertinent regulations. Concerning the concrete questions you put in front of us, as other Delegations said, the term was activated. We could agree that it would be a useful or could be a useful qualifier. Then we would like to see the incremental approach and continuity of identify, select and engage, retain, especially since identification is in our view an important element of laws. Also in relation to the potential bias, it could include as our colleagues from the German Delegation put out furthermore, we also would prefer the term human intervention as it is broader and reflects better the chain of command and control of the system. On deploying force or applying force, we were the ones that in our national submission used or submitted the term deploying force where we would, of course, be okay with the term applying force as it seems to have a broader support in the room. Finally, we are, of course, happy to discuss topic 2 and 3 during the intersessional period. I thank you.","[...] Then we would like to see the incremental approach and continuity of identify, select and engage, retain, especially since identification is in our view an important element of laws. Also in relation to the potential bias, it could include as our colleagues from the German Delegation put out furthermore [...]",,,,x,,,,,,,,,,,,,,,
BULGARIA,12:38:33,2024-03-08,0:01:54,"Thank you, Chair. I apologize for jumping in in the last minute but due to some untypical force major circumstances I only received your paper this morning. So I have some short remarks. Thank you for successfully guiding us through the first week and this revision definitely helps narrowing down our work. On the question of once activated, we do not consider it to be superfluous concept. On the contrary, it is one of the relevant aspects of laws, especially in connection with human intervention. The US proposal of after activation I guess also works for us. On identify, select and engage, we would like to go for all three of these together. As Germany said and shared by Luxembourg, identification is important especially in relation to potential biases. We would consider select and engage to be cumulative as Australia and ICRC suggested. For identification, we could have a similar position. However, we would like to give it a further consideration on this issue. We would go for apply force and as we said earlier this week, we were convinced by others that the term human intervention in combination with the word further is more appropriate option. When it comes to characterization, we are also willing to give further consideration to the ICRC proposal for inclusion of human user as more broader that includes not only operators but also other persons within the chain of command. Thank you, Mr. Chairman.","[...] On identify, select and engage, we would like to go for all three of these together. As Germany said and shared by Luxembourg, identification is important especially in relation to potential biases. [...]",,,,x,,,,,,,,,,,,,,,
EUROPEAN UNION,11:37:36,2024-08-29,0:03:46,"Thank you, Mr. Chair. The EU and its Member States would like to make the following suggestions of the elements in this section. First, the scope for prohibitions in the future set of elements of an instrument and other possible measures should include the following. First, development, deployment and use of those systems which operate outside of human control and responsible chain of command. Secondly, those systems which effects cannot be limited, anticipated or controlled as required by IHL in the circumstances of the ir use and which would result in incidental loss of civilian life, injury to civilians and damage to civilian objects or a combination thereof which would be excessive in relation to the concrete and direct military advantage anticipated. Third, those weapon systems which are of a nature to cause superfluous injury or unnecessary suffering or are incapable of being used in accordance with IHL such as the principles of distinction and proportionality as well as the obligation to take feasible precautions in attack. And last, those systems which are inherently indiscriminate and / or are incapable of being used in accordance with IHL including the principle of distinction between civilians, combatants and persons of the combat. With regard to the scope of the regulations, the future set of elements of an instrument and other possible measures should include the following possible requirements to ensure that little autonomous weapon systems are being used in accordance with international law, in particular IHL and relevant ethical considerations. Firstly, in the study, development, acquisition or adoption of a new weapon means or method of warfare, determination must be made whether its employment would in some or all circumstances be prohibited by international law. Secondly, ensure appropriate levels of human control and judgment which must be retained during the entire life cycle of the weapon system. Thirdly, control and / or limit the types of targets that the system can engage as well as the duration, geographical scope and scale of the operation of such weapon systems such as where appropriate the incorporation of self-destruct, self-deactivation or self-neutralization mechanisms into munitions or the system. Fourthly, prevent and mitigate risks of unintended bias including their potential impact on determining combatants and other risks regarding the safety and security of such weapon systems, including the risk of diversion to unauthorized users including terrorist groups. And lastly, ensure the highest possible and appropriate level of reliability as well as the traceability of such weapon systems in order to securing commanders and operators' responsibility and accountability. Thank you, Mr. Chair.","[...] Fourthly, prevent and mitigate risks of unintended bias including their potential impact on determining combatants and other risks regarding the safety and security of such weapon systems, including the risk of diversion to unauthorized users including terrorist groups. [...]",,,,x,,,x,,,,,,,,,,,,
ICRC,11:06:56,2024-08-30,0:03:54,"Thank you, Mr. Chair. Good morning, colleagues. The ICRC welcomes this section which sets out concrete steps that states will need to take to be able to ensure that their development and use of laws will comply at all times with international law. Before I go on, it is not often that we get the chance to agree with the Russian Federation. So I would like to thank the Distinguished Colleague from Russia for making a proposal on paragraph 1 that rigorous testing should be in the context of real life operations. We could certainly support that improvement. Looking at the section as a whole, however, we think it needs some clarification to distinguish legal reviews from technical reviews of capabilities, effects, bias and so on. Both these kinds of reviews are important and indeed technical reviews would form a necessary component of any legal review. But we recommend that the group maintain a clear distinction between the two in order not to conflate existing obligations under international law with the technical reviews introduced in this text. Legal reviews are a critical measure to ensure that the employment of autonomous weapons systems will not be in violation of IHL and other applicable international law and are a legal obligation under additional protocol 1 to the Geneva Conventions. We therefore propose a new paragraph on legal reviews based on the agreed language of the GGE as has been recognized in the GGE's guiding principle E and included in the GGE's 2023 consensus report in paragraph 23 of that report. Furthermore, to make this paragraph of the most particular relevance to autonomous weapons systems and based on previous discussions in the GGE and the increased convergence on this subject, we propose to include the clarification that a new review is required once a weapon system is modified. So we would propose a new first paragraph that would introduce this section and would read as follows and we have provided this in writing to your team, Mr. Chair. It begins in accordance with the state's obligations under international law in the study, development, acquisition or adoption of any laws including laws that result from modifications of existing non-autonomous weapon systems, determination must be made whether its employment would in some or all circumstances be prohibited by international law. So that first paragraph would deal with the legal reviews. The following paragraphs would then refer more to the technical reviews and so we would then suggest deleting the phrase conduct legal reviews of laws to from the second paragraph. So paragraph 2 would then read states should understand the weapon's capabilities and limitations, expected circumstances of use and its anticipated effects in different circumstances. Finally, as we mentioned yesterday in the discussion on prohibitions and restrictions, we suggest placing here in this section the paragraph on training and instruction. So we recommend the group takes that paragraph from the previous section and add it as a new final paragraph here which would read states must ensure appropriate training and instruction for human users of laws. Thank you, Mr. Chair.","[...] The ICRC welcomes this section which sets out concrete steps that states will need to take to be able to ensure that their development and use of laws will comply at all times with international law.[...] Looking at the section as a whole, however, we think it needs some clarification to distinguish legal reviews from technical reviews of capabilities, effects, bias and so on. Both these kinds of reviews are important and indeed technical reviews would form a necessary component of any legal review. But we recommend that the group maintain a clear distinction between the two in order not to conflate existing obligations under international law with the technical reviews introduced in this text. Legal reviews are a critical measure to ensure that the employment of autonomous weapons systems will not be in violation of IHL and other applicable international law and are a legal obligation under additional protocol 1 to the Geneva Conventions.  [...]
",,,,x,,,,,,,,,x,,,,,,
BRAZIL,11:17:44,2024-08-30,0:02:40,"Thank you, Chair. The language of this section is quite a good basis for our discussions. We would have some minor some slight suggestions and amendments. First, turning to paragraph 1, like Austria, I think that the suggestion by the Russian Delegation is positive. Although I would believe that the best term I believe that the proposal was including in real operating environment. I believe that the best term would be realistic. Otherwise, we would probably be asking for testing evaluation to be done in real conflict situations which I do not think that would be the intention there. If that is the case, we welcome the proposal including in realistic operating environments. On paragraph 2, I believe that the proposals made by the ICRC and Austria in order to clarify that we also that there is also a need for legal reviews in case of further development of existing systems or changes to their algorithms. I think it is important. I would also propose that we include a reference to an encouragement to states to exchange experiences and best practices on criteria and procedures for such legal reviews. And finally, we would propose a new paragraph 4 BIS with a reference to technical standards. It would read states should consider developing technical standards that can serve as a reference for the development of laws with a view to promoting safety, security, integrity, and compliance with IHL requirements. I will send this to you, Mr. Chair, and to your team. This is tied to our working paper that we tabled last session, the last session earlier this year, highlighting the possible contribution to the work of the GGE of the IEEE standard for ethically driven robots and automation systems, IEEE 7007-2021. Thank you very much, Chair.","[...] On paragraph 2, I believe that the proposals made by the ICRC and Austria in order to clarify that we also that there is also a need for legal reviews in case of further development of existing systems or changes to their algorithms. I think it is important. I would also propose that we include a reference to an encouragement to states to exchange experiences and best practices on criteria and procedures for such legal reviews. [...]",x,,,,,,,,,,x,,x,,,,,,
IRELAND,11:36:12,2024-08-30,0:02:22,"Thank you, Chair. Thank you for all the work you and your team have done throughout the week. We would just like to echo support for the suggestions that have already been made by various Delegations on paragraph 1 in terms of addition of reference to real life operations which we think would bolster this section. We think there is definite merit in that. We would also support the suggestions put forward by the ICRC in terms of making sure we have clarity on the types of reviews we are referring to and we would also see merit in the additional paragraph that they have suggested for inclusion as well. We think that would be a welcome addition. Likewise, the Austrian suggestion on constant assessment and consideration of machine learning and reflection of any changes that may appear on first listen, this again is something that would have merits. We look forward to seeing that in writing and engaging on some formulation of that. Echo comments made by other Delegations on paragraph 3. We believe there is a need to strengthen the language here beyond mere detection as it is currently framed. In this vein, we would suggest consulting the joint working paper submitted in the previous session by Ireland and nine other states as already referenced by our colleague here from Panama and in particular the recommendations outlined in paragraph 11 of that working paper. Similarly, we would echo the comments by Panama, Austria and others on concerns with any qualifiers regarding bias which we would like to point out already has an inherent negative connotation at source. Similarly in paragraph 4, we believe something is required alongside reduced to strengthen this portion. We note the suggestion made a few minutes ago from Switzerland on seeking eradication as much as is possible. So this could be a basis moving forward. Similarly, we also think there is a potential to add mitigate to this list to further outline the steps that would be required. Likewise, we want to echo support for reference to important training and instruction as outlined in the suggestion to the ICRC. This is an important component to pursue and would bolster this section overall. Finally, at first listen, the Brazilian language proposals also merit consideration and possible pursuance. I believe this could be a basis for something that we could include and refine further. Thank you, Chair.","[...] Similarly, we would echo the comments by Panama, Austria and others on concerns with any qualifiers regarding bias which we would like to point out already has an inherent negative connotation at source. [...]
",,,,x,,,,,,,,,,,,,,,
COSTA RICA,11:44:11,2024-08-30,0:02:41,"Thank you, Chairperson. We will be brief. We want to support the ICRC's proposal to add a preparagraph to Paragraph 1 and we also wish to express support by what was said by Panama, Austria and Ireland in particular the need to specify testing as suggested by Brazil. There is a need to be more specific and perhaps require it to be carried out in conditions that are close to the real life environment. In Paragraph 3, we have listened attentively to the proposals to improve the paragraph. We believe this is one of the most important paragraphs from my Delegation that is the case and we believe that bias needs to be looked at more closely because Costa Rica is one of a large group of countries that have come together to prevent a working document seeking and we would wish to see more elements from that working document reflected in Paragraph 3. On Paragraph 4, we welcome the explanation from Panama. This does reflect the importance of a comprehensive approach to the topic before us. We are addressing obligations incumbent on all States' parties and we cannot limit our approach too much. We are speaking of erga omnes obligations as Panama has stated. So we need to amend this wording in Paragraph 4 and we would call for a more comprehensive approach in addressing this matter. Thank you.","[...] In Paragraph 3, we have listened attentively to the proposals to improve the paragraph. We believe this is one of the most important paragraphs from my Delegation that is the case and we believe that bias needs to be looked at more closely because Costa Rica is one of a large group of countries that have come together to prevent a working document seeking and we would wish to see more elements from that working document reflected in Paragraph 3. [...] ",,,,x,,,,,,,,,,,,,,,
CANADA,11:53:41,2024-08-30,0:01:19,"Thank you, Chair and good morning, colleagues. Just a few comments from my Delegation on this section. First on paragraph 1, we have no issues with the proposals made this morning regarding the addition of realistic scenarios or real life operations. On paragraph 2, we are also supportive of the recommendation that was made by another Delegation to add the encouragement of exchanges of best practices between states. On paragraph 3, which is very important to my Delegation and we wish to see more language pulled from the joint working paper on biases that was presented at the last GGE session in March, specifically from the recommendations outlined in paragraph 11. And finally, just to add, we are also supportive of adding or pulling down the paragraph on training from the earlier section as a paragraph 5 for this section. I thank you.","[...]On paragraph 3, which is very important to my Delegation and we wish to see more language pulled from the joint working paper on biases that was presented at the last GGE session in March, specifically from the recommendations outlined in paragraph 11.[...]",,,,x,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,
RUSSIAN FEDERATION,16:32:22,2024-03-04,0:07:00,"Thank you very much. Thank you very much, Distinguished President. At the outset, I would like to join my colleague from the Republic of Korea in expressing our gratitude for your attempt to prepare such a document, an attempt to consolidate the different views or opinions that were expressed by different Delegations during this work of the GGG that has been going on for quite some time, work related to definitions and characterizations of what we are discussing here. We would like to share with you our very preliminary considerations in connection with what we see on the screen because this is the first time we are looking at this document and it is the first time we are considering the proposals that are contained in the document. Of course, all of our discussions right now are really preliminary in their nature, very general, and we believe that the results of this discussion will require really serious further work back in capital by all Delegations. Now, our preliminary remarks on the bullet points that you are proposing, we fully realize that any attempt to bring different approaches together will in one way or another will lead us to a certain bias in favor of one concept, one notion or another one. Right now, as we see it, the notion of human control is taken as the main idea and, in fact, I would refer to meaningful human control and as well as also trying to come up with a definition through the critical functions of the system. Now, we do not fully share that approach. We believe that we should try to take a step-by-state approach as we try to agree on a definition of such systems. First, what we need to do is to define what types of weapon systems we are talking about. We discussed that for quite some time and our colleague from the Republic of Korea also referred to that fact when he tried to say that most likely we should be talking about lethal systems and we believe that we have to discuss this topic as well to discuss the topic of what type of weapon systems we are referring to. A second very important factor as we try to come up with such a definition relates to the objectives that these systems are pursuing and this refers first and foremost to the third and fourth bullet point. Here we see that there are certain objectives that are listed and we see that this is done in a very narrow manner, but in any case, we are ready to discuss these objectives that could be pursued by such systems that would fall under the definition of emerging technologies in the area of laws and third important point or factor and it relates to human control. We believe that we will be discussing the question of human control in a detailed fashion under the second agenda item that you have submitted to the GGE and in a very preliminary way we would like to say that we find that trying to identify the specific functions where human control will apply is something that is counterproductive and not really advisable because of the different views that Delegations have on that subject and also this relates to the factor of the responsibilities that states have in complying with their international obligations including in the area of IHL where there is a clear definition of the responsibility of states' parties for ensuring relevant human control related to the use of different weapons systems and this obviously refers to the entire life cycle of such weapons systems and that understanding if I am not mistaken is something that we agreed to on the basis of consensus as one of the guidelines and also as one of the conclusions that this GGE made in the past. Thank you."," [...] Now, our preliminary remarks on the bullet points that you are proposing, we fully realize that any attempt to bring different approaches together will in one way or another will lead us to a certain bias in favor of one concept, one notion or another one. Right now, as we see it, the notion of human control is taken as the main idea and, in fact, I would refer to meaningful human control and as well as also trying to come up with a definition through the critical functions of the system. [...] ",,,,x,,,,,,,,,,,,,,,
CUBA,17:17:34,2024-03-04,0:03:52,"Thank you, Chair. We as a country have a few ideas that we would like to share with everyone about this section on curiosity and restrictions and definitions. We will be addressing the ideas that have been presented on the screen later on. To begin, for our Delegation, an agreed upon characterization of autonomous weapons is very useful in order to fulfill the existing mandate, which is why we appreciate this approach. Our Delegation understands that this is an important issue and one that cannot be set aside. However, it cannot be used as a pretext either in order to avoid carrying out negotiations. Existing protocols within the Convention must be seen as good examples to follow. We believe that an exercise as the one we are currently engaged in with definitions and characterizations must consider completely autonomous weapons as well as semi -autonomous weapons. For our Delegation, autonomous weapons must be seen as those that act without human involvement once they are deployed. Semi -autonomous weapons for their part are to be understood as those that have human oversights, at least during the critical stages, that is for the selection and the attack of targets, as is found in the working document of our Delegation from 2016. Military drones among other weapons with certain level of autonomy that are already being used must also be considered as semi -autonomous weapons and ought to be effectively regulated. Seizing control, responsibility and human involvement in decisions on the use of force, algorithms and controlled processes online raise fundamental ethical issues. It is obvious for our Delegation that human involvement and oversight lie at the very core of future characterizations of these systems and they determine whether or not they can be developed or not in the first place and in the second, how they ought to be regulated. It is for these reasons that we believe that to reach common understandings as part of this exercise on concepts such as significant human involvement can also be most useful in our efforts. Turning now our comments to the proposals found on the screen, we would like to express our appreciation to you, your team and the Secretariat for having prepared them. Quite preliminarily we can say that we do see merits in the inclusion of this set of characterizations of the three following points. First, the critical functions of selection and attack of targets as these are a component of the characterization that we agree with in our national capacity. Secondly, apply force as a continuance of the previous idea and an important part of significant human intervention. Third, the notion of human intervention, which is important. We believe that keeping the human component is essential were we to consider other proposals such as an operator involvement. For our Delegation, it will be important to consider a potential characterization in the context of a whole host of elements of broader instruments or measures and bearing in mind that future regulations would take these characterizations into account. Thank you.","[...] Seizing control, responsibility and human involvement in decisions on the use of force, algorithms and controlled processes online raise fundamental ethical issues. It is obvious for our Delegation that human involvement and oversight lie at the very core of future characterizations of these systems and they determine whether or not they can be developed or not in the first place and in the second, how they ought to be regulated. [...]",x,,,,,,,,,,x,,,,,,,,
SWITZERLAND,12:05:20,2024-03-05,0:04:55,"Thank you, Mr. Chair. We just have a few observations, a few overall observations at this point. We are following the discussion closely and try to really understand the nuances that we are in now. But broadly speaking, we believe it is really absolutely key that we build on the work so far, build on the agreements that we have had past years. Autonomy is a spectrum, not an either / or and that should guide how we discuss today the points that are on the slide here. We also would join all others who would warn from overly restrictive approaches in general but also at this stage where we are in the debate. We like what the UK said a moment ago that actually what we are doing here is we are painting a picture based on which we then can base the regulation approach. I think the Austrian Delegation made a very good point also that a lot of the things here can be left for later. This is actually some of what we are talking about now. We can address separately further down when we talk about an instrument, when we talk about prohibitions and regulations. On these specific points on the slide, we quite like the Brazilian contribution, the degree in which a system can perform critical functions without human intervention. This picks up the critical functions. This picks up the discussion we had about human intervention. It reflects that it is not black or white. It is not an on and off. It is a differentiation that is needed. We would consider this dimension is really at the core of what we seek to capture in the context of this GG. In a self-sufficient or self-governing manner, these terms to us seem unnecessarily complicated. What if a system is not throughout self-sufficient but only in certain phases? A hypothetical example, what if another system, a system B, so to say, governs the autonomy of system A? We should not include overly complicated concepts here. The same applies with the notion about programming sensors and algorithms. That is a good description, good enumeration of things, but here we see the risk that we are looking at this in a cumulative manner. Does it really need to have all these components? We see, of course, an interest in qualifying the technology, but there is the risk that we may miss some systems that should fall under the instrument that we are talking about here. This is even more the case if we seek to have a characterization that can also stand the test of time. That was said a number of times also yesterday. We need to make sure this can develop with the technology over time. We must make sure that we do not exclude certain aspects, certain technological developments that we might not yet be aware of yet. This is a more general point, again, on the cumulative approach. We are really not quite sure. I think it is a good discussion that we should have this week, notably with the critical functions, whether we really need and, or to what extent engage as such is really the critical point. The others are, of course, also important, but we should maybe also at this stage as a working hypothesis also think of and / or approaches. As I said at the beginning, we are following very closely. These are really good discussions that help us getting clarity, conceptual clarity that is much needed on that point. Thank you so much.","[...] This picks up the critical functions. This picks up the discussion we had about human intervention. It reflects that it is not black or white. It is not an on and off. It is a differentiation that is needed. We would consider this dimension is really at the core of what we seek to capture in the context of this GG. In a self-sufficient or self-governing manner, these terms to us seem unnecessarily complicated. What if a system is not throughout self-sufficient but only in certain phases? A hypothetical example, what if another system, a system B, so to say, governs the autonomy of system A? We should not include overly complicated concepts here. The same applies with the notion about programming sensors and algorithms. That is a good description, good enumeration of things, but here we see the risk that we are looking at this in a cumulative manner. Does it really need to have all these components? We see, of course, an interest in qualifying the technology, but there is the risk that we may miss some systems that should fall under the instrument that we are talking about here. [...]",,,,,,,,,,,x,,,,,,,,
UNITED STATES,12:13:02,2024-03-05,0:10:11,"Thank you, Mr. Chair. Just wanted to provide my Delegation's reactions on this question, how could autonomy be described or explained? First, I just wanted to draw colleagues' attention to what I thought was a very good explanation by our colleague from PACS about autonomy. I hope colleagues heard it. It just these and other contributions from civil society really show the value of the GGE as an inclusive process with active participation by civil society, ICRC and others and I thought an excellent point that our colleague from PACS made was that autonomy can be understood as a way of using the system, that we are thinking about autonomy both as a capability that the system has, but it is also a way of using that capability and sometimes you could have a capability that is used one way and it is autonomous but used in another way, it is semi -autonomous or not fully autonomous. I think that is an important idea for us to keep in mind as we are thinking through this issue of how can autonomy be described or explained. In our response to you, Mr. Chair, with regard to this question, we said that in the context of weapon systems, autonomy can refer to the capability of the weapon system to perform various functions without human intervention. It can also refer to the way in which the system is used. In other words, the operator relying on the system to perform those functions without the operator performing them. Another way of putting that idea sort of more in line with the framing of your question, autonomy can be described or explained as a capability of the system and reliance upon that capability by an operator to perform a given function without operator intervention. Autonomy can be described or explained as a capability of a system and reliance upon that capability by an operator to perform a given function without operator intervention. So that is one way of thinking about that idea and trying to capture both dimensions of autonomy, both as a capability and as a way of using the system. Just some very specific reactions now to the bullets that you have assembled under this question. With regard to the first bullet of specific functions of weapon systems, we were struggling a bit to understand that we see autonomy as a general concept that can apply outside the context of weapon systems. So we see this more as a contextual element of this idea rather than as an explanation of what autonomy is. With regard to the second bullet, the capability to perform a given task, I think we would like to be more specific as we are trying to be precise and to sketch out an instrument that we think can guide states' behaviors. We want to be as precise as possible to reflect clear common understandings. So we would add in the notion of a system, the capability of a system to perform a given function. I think our preference would be for function rather than task because that is what our group has focused on and found consensus on is functions in systems. And then this notion of without operator intervention. And here it is similar to task, to language in the next bullet. You know, there's been some discussion is it better to have human intervention or operator intervention. We have tried to explain that we think operator intervention is more clear and we are excluding the possibility of people who are not the operator in question intervening and somehow affecting the operation of the system. From our perspective, the notion of human operator is redundant, right? We view operator as necessarily meaning human beings and usually in instruments we do not have the word human before every term that refers to a human being, but we just understand that instruments are referring to human beings. I think we are open to hearing more discussions about why people think human is necessary, but from our perspective that is already contained and superfluous to add. I did also want to just respond on the notion of critical functions again in this bullet. We have explained our comment previously, you know, this concept is a bit unclear. We appreciated the response from the Delegation of Brazil. It was very thoughtful and appreciated them trying to explain their thinking. One, I think, initial reaction to that in the spirit of interactivity is that it does not make sense from our perspective to put legal concepts inside the definition. We recommend that the definition or characteristics be as factual as possible so they can be as clear as possible so we know what we are talking about. If critical here means critical to compliance with IHL, then that is a very complex legal idea and, you know, to take the example that Brazil offered navigation, we actually think navigation could be very critical to compliance with IHL. A lot of systems will navigate to a particular area. A system might be used in an area constituting a military objective or certainly munitions can be directed against locations that are military objectives and so in that kind of usage, the notion of navigation which Brazil said would be excluded, you know, actually would be included from our perspective because it is relevant to IHL compliance. I think our perspective is more similar to what we are mentioning, the view that we thought Austrian colleagues were expecting. Let's deal with those substantive issues later but here let's just try and be factual and clear in what kind of systems are under discussion. With regard to the next bullet, subject to different degrees and types of human control, I think you know my Delegation's positions with regard to the terminology of human control, here, you know, we do not think that this is really an explanation of autonomy and, of course, you know, we do not want to endorse the notion that autonomy is necessarily the opposite of control or that autonomy leads to less control. In fact, as we have noted before, autonomous functions can assist the operator in controlling the effects of the weapons system better than without such capabilities. With regard to the next bullet, in a self-sufficient or self-governing manner, here, you know, we did not embrace this terminology. You know, from our perspective, it seemed somewhat inconsistent with our guiding principle of not anthropomorphizing machines. We need to be careful not to say that machines are people, you know, so self-governing to us sounded more like what people do rather than machines. And then lastly, you know, with regard to the last bullet, programming sensors and algorithms, you know, I think as a first note, the antecedent of the word "" its "" in this bullet is somewhat unclear because it does not refer to anything else prior in the sentence. And then, you know, I think we would regard programming sensors and algorithms as technical components, but they do not necessarily explain autonomy. In our view, autonomy is how these components form a capability and how that capability is used. But not necessarily these kind of technical pieces. And then lastly, just with regard to algorithming pardon me, algorithms and programming, we see algorithms as part of programming and so would question why those are set out as distinct. I thank you, Mr. Chair.","[...] And then lastly, you know, with regard to the last bullet, programming sensors and algorithms, you know, I think as a first note, the antecedent of the word "" its "" in this bullet is somewhat unclear because it does not refer to anything else prior in the sentence. And then, you know, I think we would regard programming sensors and algorithms as technical components, but they do not necessarily explain autonomy. In our view, autonomy is how these components form a capability and how that capability is used. But not necessarily these kind of technical pieces. And then lastly, just with regard to algorithming pardon me, algorithms and programming, we see algorithms as part of programming and so would question why those are set out as distinct. [...]
",,,,,,,,,,,x,,,,,,,,
HOLY SEE,10:57:49,2024-08-26,0:03:09,"Mr. Chair, at the outset, please allow me to thank you for all the preparatory work that you have conducted in advance of the second session of the GGE. In particular, this Delegation wishes to thank you for the rolling text that you have provided, which constitutes a valuable foundation upon which to build the shared understanding. Speaking to the G7 leaders gathered in Italy last June, Pope Francis urged to reconsider the development and use of devices like the so-called lethal autonomous weapons and ultimately ban their use. This starts from an effective and concrete commitment to introduce even greater and proper human control. No machine should ever choose to take the life of a human being. For the Holy See, given the pace of technological advancements and the research on weaponization of AI, it is of the utmost urgency to deliver concrete results in the form of a solid legally binding instrument and in the meantime to establish an immediate moratorium on their development and use. In this regard, it is profoundly distressing that adding to the suffering caused by armed conflicts, the battlefields are also becoming testing grounds for more and more sophisticated weapons. Mr. Chair, this Delegation appreciates the references to both appropriate control and human judgment in your rolling text, although we would welcome more clarity and common understanding of these terms. In this regard, it is useful to recall the difference between a choice and a decision. While pointing out that machines merely produce technical algorithmic choices, Pope Francis recalled that human beings, however, not only choose but in their hearts are capable of deciding. A decision is what we might call a more strategic element of a choice and demands a practical evaluation. Moreover, an ethical decision is one that takes into account not only an action's outcomes but also the values at stake and the duties that derive from those values. Mr. Chair, the Holy See deems it of fundamental importance to retain references to human dignity and ethical considerations at the core of our deliberations. It is necessary to ensure and safeguard a space for proper human control over the choices made by AI programs. Human dignity itself depends on it. In this regard, this Delegation welcomes the prominent role given to ethical consideration at the recent conference, humanity at a crossroad, which was held in Vienna on 29/30, April 2024. These and other similar conferences on the same subject are further indication of an ever-growing awareness of the ethical concerns raised by the weaponization of AI. Such public awareness represents a remarkable ever-growing conscience public that cannot be ignored. In conclusion, the development of ever more sophisticated weapons is certainly not the solution. The undoubted benefits that humanity will be able to draw from current technological progress will depend on the degree to which such progress is accompanied by adequate development or responsibility and values that place technological advancement at the service of integral human development and of the common good. Thank you, Mr. Chair.","[...] Mr. Chair, this Delegation appreciates the references to both appropriate control and human judgment in your rolling text, although we would welcome more clarity and common understanding of these terms. In this regard, it is useful to recall the difference between a choice and a decision. While pointing out that machines merely produce technical algorithmic choices, Pope Francis recalled that human beings, however, not only choose but in their hearts are capable of deciding. A decision is what we might call a more strategic element of a choice and demands a practical evaluation. Moreover, an ethical decision is one that takes into account not only an action's outcomes but also the values at stake and the duties that derive from those values. [...]",x,,,,,,,,,,x,,,,,,,,
FUTURE OF LIFE INSTITUTE,12:41:57,2024-08-26,0:02:20,"The Future of Life Institute thanks you for your continued leadership of this group and we equally commend the work of the Secretariat in facilitating and informing our work. Over the past year we are seeing unprecedented acceleration in the development, sale and use of autonomous weapons. While these weapons remain unregulated, civilians bear the brunt of their use. Momentum towards regulation has never been higher as we saw the ECOWAS Conference in Sierra Leone and the convening of 144 states at Austria's Humanity at the Crossroads Conference. We commend the demonstration of leadership on this issue and in this vein we encourage states to continue to associate with the Austrian Chair's summary as a signal to demonstrate global convergence in support of a legally binding instrument. Despite the complexities that emerge when considering using AI to inflict violence, some simple facts remain. The vast majority of citizens are horrified by autonomous weapons and do not want to see algorithms make the decision to kill. These sentiments were reflected in the recent calls from Pope Francis to the G7 on autonomous weapons. We are highly encouraged by the input and conclusions from the Secretary General's report. The messages are clear. Time is running out to stem the harmful effects of weapons that continue to be politically unacceptable and morally repugnant. We share the Secretary General's call to states to utilize all forums available to regulate autonomous weapons, in particular the UN General Assembly as an appropriate forum to address all risks posed by autonomous weapons in a comprehensive manner. Allowing more states to participate in these discussions can only help to represent all voices and perspectives. We share the Secretary General's conclusions that autonomous weapons present risks that extend beyond IHL. Ignoring these elements would risk working with a severely incomplete understanding of how these weapons will impact society. This work is not easy and we at the Future of Life Institute would like to assist states and civil society alike. For this reason we have produced the Diplomats' Guide to Autonomous Weapon Systems for those new to the subject and equally for those well versed. We thank you. We remain engaged and committed to engage with states to safeguard international peace and security and set strong norms surrounding life and death decisions. Thank you. Let me thank you for your intervention.",The vast majority of citizens are horrified by autonomous weapons and do not want to see algorithms make the decision to kill. ,,,,,,,,,,,x,,,,,,,,
UNIVERSITY OF CAMBRIDGE,11:55:15,2024-08-30,0:05:02,"Since this is my first time taking the floor since you took this role, I would like to congratulate you and thank you and the rest of the Secretariat for their hard work and support. We all know it is no easy feat. On testing and evaluation and review, I would remind us all that additional protocol 1's Article 57.1 provides that constant care must be taken. It might serve us to refer to the 2018 CCW Report in which 23A stated, "" Humans must at all times remain accountable in accordance with applicable international law for decisions on the use of force, "" and 23C, "" Weapons systems under development or modification which significantly changes the use of the existing weapons systems must be reviewed as applicable to ensure compliance with IHL. "" The current text before us, as mentioned by many, lacks the sufficient provision for such dynamic systems which might alter post-deployment behavior. My colleague at ENCODE Justice earlier this week mentioned the issue of decay which we might refer to as AI aging or degradation. AI systems in particular are dynamic, not static, and susceptible to AI aging immediately upon deployment. Indeed, a recent study found that 91 % of machine learning models degrade in time. Post-deployment data drift is a good example of environmental data gathered by field centers, altering post-deployment object recognition by gradually introducing a missing class into the data stream. Gradual drift is notoriously hard to detect. Meanwhile, change detection tools programmed within the system cannot resolve change once detected, creating techno legal issues. Repeated testing is therefore required for alignment with command and control intent and IHL. I was lucky to assess a month or two ago a taxonomy of AI risk produced by MIT which I believe is to be published as we speak if not before. It showed the 10 % AI risk factor across multi-sector domains increased to a 65 % AI risk profile post-deployment. We know that when it comes to military use, risk factors are implicitly higher due to the chaotic environments involved. I would remind the room that even the most convincing test results of a system can be flawed from reward hacking which sees systems bypass necessary training steps to achieve optimum outcomes in their training, misdirecting the user or failing to encode vital compliance steps to full on deception. META's Cicero and DeepMind's AlphaStar exhibited AI systems engaging in premeditated deception. Where META's creators intended their game diplomacy to behave honestly, in fact, and I quote, it not only betrayed other players but also engaged in premeditated deception, planning in advance to build a fake alliance with a human player in order to trick that player into leaving themselves undefended for an attack. I would alert you to the fact that there is recent alarm over the fact that as a University of Oxford team found, and yes, we constantly refer to each other's work, that using AI generated data sets to train future models may generate nonsense, a concept known as model collapse. I would alert you to the fact that, and this is rarely talked about, a human team achieved a more than 97 % win rate against the much celebrated AI system Catago running at its super human settings. Their adversarial policy did not beat it by playing go well. It beat it by tricking Catago into making simple but serious mistakes. Let's not forget environment misrepresentation created by sensor degradation in the field would change a system's analysis and response, not to mention that transference to nondesignated environments might render a technology utterly noncompliant. I bring these examples to remind us that the provision of testing and evaluation and review language alone would be woefully inadequate without the qualifiers which have been mentioned before, such as ongoing or continual. For those concerned about the scope of such language, this could be qualified using the 2018 language to be associated specifically with dynamic systems using weapon systems under development or modification which significantly changes the use of existing weapon systems must be continually reviewed. I may refer back with some separate comments on bias if you will indulge me. Thank you.","[...] I bring these examples to remind us that the provision of testing and evaluation and review language alone would be woefully inadequate without the qualifiers which have been mentioned before, such as ongoing or continual. For those concerned about the scope of such language, this could be qualified using the 2018 language to be associated specifically with dynamic systems using weapon systems under development or modification which significantly changes the use of existing weapon systems must be continually reviewed. I may refer back with some separate comments on bias if you will indulge me. [...]",,,,x,,,,,,,,,,,,,,,